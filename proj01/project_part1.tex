%!TEX TS-program = pdflatex or xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass{article}
\usepackage[letterpaper, margin=1in, left=0.75in, right=0.75in] {geometry}
\usepackage{microtype} % micro appearance
\usepackage[all]{nowidow} % no lone lines
\usepackage{changepage} % changes layout mid-page
\usepackage{enumitem} % enum (itemize, enumerate, description)
\usepackage{ifthen}
\usepackage{xfrac}
\usepackage{fancyhdr} % headers
\usepackage{amsmath} % matrices
\usepackage{amssymb} % symbols
\usepackage{gensymb} % symbols
\usepackage{tikz} % graphics?
\usepackage{bm} % bold math
\usepackage{multicol} % multiple columns
\usepackage{listings} % listing lines of code/output
\usepackage{setspace} % 1.5 spacing

\pagestyle{fancy}

\newcommand {\classname} {CS 4740/5740 -- Intro to Natural Language Processing}
\newcommand {\duedate} {Mon, February 24}
\newcommand {\hwtype} {Project}
\newcommand {\hwnum} {1: Language Modeling}
\newcommand {\innermargin} {0.15in} % indentation amt of nested lst

\let\tss\textsuperscript
\let\oldunderline\underline
\renewcommand\underline[1]{\oldunderline{\smash{#1}}}

\fancyhead[L] {\ifthenelse {\thepage=1}
  {bgs53, sg754, amw275\\
   Ben Shulman, Spandana Govindgari, Andy Wang}
  {\hwtype\ \hwnum\ (bgs53, sg754, amw275)}}
\fancyhead[R] {\ifthenelse {\thepage=1}
  {\classname\\\hwtype\ \hwnum\ (due \duedate)}
  {Page \thepage}}

\begin{document}
\begin{center}\textbf{Project 1 Part 1\\Progress on unigram and bigram table construction algorithms}\end{center}

All our code is written in Python. We use several libraries, including the nltk platform's punkt for tokenization.\par

Before we inputted the training data for both corpuses into our ngram implementation, we wrote a script to preprocess them, removing extraneous xml-like tags, numbers, etc. For the bible corpus, our sentences are split on Bible verses of the format \texttt{[0-9]+:[0-9]+}. Ben and Spandana has written much of the preprocessor for the Bible and Hotel corpuses respectively.\par

For our ngram model, we initially began by individually creating a unigram and bigram model class, but decided that there was already much in common between the two systems, so an abstraction was not too hard to make. Hence we went immediately to an n-gram model. Our ngram class reads in the preprocessed sources, and splits the text at spaces.\par

Our storage is a list of dictionaries, each of which store unigram/bigram/trigram/etc data. The dictionaries record counts of the $p$ previous words followed by a word, i.e.\ for a bigram model, it stores the unigram counts and bigram counts. Each dictionary then holds an entry (another lookup table) for each tuple of previous words, i.e.\ for unigram there is only one entry: \texttt{[((), [(the,5),(a,6),(cat,2),...])]}. For bigram, there would be \texttt{[('the',[('cat', 3),('dog', 4),...]),('a',[(cow, 2),(horse, 1),...]),...]}\par

Here is a snippet of code in our ngram class that is called by its constructor to initialize the ngram table. Essentially, we can step through the corpus word-by-word, and accumulate a list of previous words (no longer than $n$) to use as a lookup. After this has been set, an optional smoothing algorithm (like add-one, or Good-Turing) follows. Probabilities are calculated last by going over every row of counts and calculating the conditional probabilities for each row.

{\small
\begin{verbatim}
    def _initializeNgram(self):
        for i in range(len(self.corpus)):
            word = self.corpus[i]
            prevs = list()
            # construct all ngrams at once
            for j in range(self.n):
                # if i-j, handle first words (when i < n), don't look too far back
                if i-j >= 0:
                    # Add another word to the previous words
                    if(j > 0):
                        prevs.append(self.corpus[i-j])
                    # Must convert to tuple to hash into dictionary,
                    # reverse list to keep words in the correct order
                    lookup = tuple(reversed(prevs))
                    if lookup in self.counts[j]:
                        if word in self.counts[j][lookup]:
                            self.counts[j][lookup][word] += 1
                        else:
                            self.counts[j][lookup][word] = 1
                    else:
                        self.counts[j][lookup] = dict()
                        self.counts[j][lookup][word] = 1
        # Smoothing will go here...

        # Generate probabilities
        # self.probs stores the probability tables (dicts of dicts) for each i-gram, for i = 1...n
        self.probs = [dict()]*self.n
        for i in range(self.n):
            ngram = self.counts[i]
            for row in ngram:
                total = self._sumDict(ngram[row])
                self.probs[i][row] = OrderedDict()
                for entry in ngram[row]:
                    self.probs[i][row][entry] = ngram[row][entry] / float(total)
\end{verbatim}}

\newpage
For Hotel reviews we used the Punkt sentence segmentation tool provided by nltk library. Punkt has a tokenizer that divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. We trained it on a large collection of plaintext in the target language before we used it. Punkt knows that the periods in Mr. Smith and Johann S. Bach do not mark sentence boundaries. And sometimes sentences can start with non-capitalized words. We made use of this sentence segmentation tool to recognize sentence boundaries. 

{\small
\begin{verbatim}
sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')

data = open('raw_reviews.train', 'w')
    for c in sents:
        # adds sentence start and end marker
         # write to file
        data.write(c+'<s> ')
    data.close()
\end{verbatim}
}

After we performed sentence segmentation, we further added spaces around punctuation marks, floating dots like \texttt{...}, and dashes like \texttt{-{}-} which were all treated as separate words. We also removed any XML and numbers IsTruthful and IsPositive.

{\small
\begin{verbatim}
edit = re.sub('(</?(TEXT|DOC)>\n)|([0-9]+,[0-9],)|((\.+\s){2,})', '', edit)
    edit = re.sub('\n|^', ' <s> ', edit)
    edit = re.sub('([,!?();:"-&/$])', r' \1 ', edit)
    edit = re.sub('(\.{1,})',r' \1 ', edit)
    edit = re.sub('--', ' -- ', edit)
\end{verbatim}
}

\newpage

\par To generate a random sentence we start with a sentence start marker. We then generate a random float (0 to 1)
and then simply traverse the appropriate row of our probability table until we reach the appropriate accumulated probability and output that word.
We continue to generate words until we generate another sentence segmentation marker.
\vspace{2mm}
\setlength{\parindent}{0cm}
\newcommand\npar{\par\smallskip}
Sample unigram sentences for the bible corpus:\npar
\texttt{<s> after ramah and , of . lord , shall this that every burnt land his my that the now all lord jabin's and few at flesh is waved he thou they of watered hath wife the unto gilead a stones might ; for ; the eliezer even above cherethites said the go of and ; that they family precious <s>}\npar
\texttt{<s> keep sin <s>}\npar
\texttt{<s> altar things ; and , : , three bags the which abraham : shall made his begat word saying snare we your i <s>}\npar
\texttt{<s> , <s>}\npar
\texttt{<s> children <s>}\npar
\bigskip

Sample bigram sentences for the bible corpus:\npar
\texttt{<s> for ashtoreth the son . <s>}\npar
\texttt{<s> david my father's house , the wheat and all the god hath sent messengers , from dophkah . <s>}\npar
\texttt{<s> and burned the land cast them , i will heal : make thee ? and the month was six hundred and the place . <s>}\npar
\texttt{<s> and thou hast found stealing any of the egyptians shall wash with him to eziongeber . <s>}\npar
\texttt{<s> beware . <s>}\npar
\bigskip

Sample unigram sentences for the hotel reviews corpus:\npar
\texttt{<s> i the ; linens . were able and our <s>}\npar
\texttt{<s> use from book fine at <s>}\npar
\texttt{<s> could said ranges smooth that course the phone expressed the its to minutes it because ; i'm we at it blamed ; <s>}\npar
\texttt{<s> faint coffee can the request were . chicago pillows fantastic intercontinental there . the the <s>}\npar
\texttt{<s> i've . mattress . at leisure a already centrally chicago don't an seem you they to the the or i booked great the find bothered time " . .... of . also above the on it . to explain my really the was online . will sub air ; good peace charge <s>}\npar
\bigskip

Sample bigram sentences for the hotel reviews corpus:\npar
\texttt{<s> we arrived at in my little lost and advertised as inside and we did inquire whether hard with this hotel in the services . <s>}\npar
\texttt{<s> we were not want or phone system " stains ; and could not a dirty and ready for business and the 10th floor ; and if you can hear they had earplugs . <s>}\npar
\texttt{<s> prior guest . <s>}\npar
\texttt{<s> there are typical - when reviewing my girlfriend are 21st century standards . <s>}\npar
\texttt{<s> the entire stay because its a room was pleased that can expect from the past the beautiful decor is saying that price . <s>}\npar


\end{document}
