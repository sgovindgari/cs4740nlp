%!TEX TS-program = pdflatex or xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass{article}
\usepackage[letterpaper, margin=1in, left=0.75in, right=0.75in] {geometry}
\usepackage{microtype} % micro appearance
\usepackage[all]{nowidow} % no lone lines
\usepackage{changepage} % changes layout mid-page
\usepackage{enumitem} % enum (itemize, enumerate, description)
\usepackage{ifthen}
\usepackage{xfrac}
\usepackage{fancyhdr} % headers
\usepackage{amsmath} % matrices
\usepackage{amssymb} % symbols
\usepackage{relsize}
\usepackage{gensymb} % symbols
\usepackage{tikz} % graphics?
\usepackage{bm} % bold math
\usepackage{multicol} % multiple columns
\usepackage{lipsum} % lipsum

\pagestyle{fancy}

\newcommand {\classname} {CS 4740/5740 -- Intro to Natural Language Processing}
\newcommand {\duedate} {Mon, February 24}
\newcommand {\hwtype} {Project}
\newcommand {\hwnum} {1: Language Modeling}
\newcommand {\innermargin} {0.15in} % indentation amt of nested lst

\let\tss\textsuperscript
\let\oldunderline\underline
\renewcommand\underline[1]{\oldunderline{\smash{#1}}}

\fancyhead[L] {\ifthenelse {\thepage=1}
  {bgs53, sg754, amw275\\
   Ben Shulman, Spandana Govindgari, Andy Wang}
  {\hwtype\ \hwnum\ (bgs53, sg754, amw275)}}
\fancyhead[R] {\ifthenelse {\thepage=1}
  {\classname\\\hwtype\ \hwnum\ (due \duedate)}
  {Page \thepage}}

\begin{document}
\begin{center}\textbf{Project 1}\end{center}
\begin{center}\textbf{Report on Language Modeling}\end{center}

All our code is written in Python. We use several libraries, including the nltk platform's punkt for preprocessing.\par

\section{Parsing text}
Our preprocessing code for both corpuses (King James Bible and Hotel Reviews) is in \texttt{preprocessCorpus.py}.\par

Before we inputted the training data for both corpuses into our ngram implementation, we wrote a script to preprocess them, removing extraneous xml-like tags, numbers, etc. Ben and Spandana have contributed to the preprocessor for the Bible and Hotel corpuses respectively.\par

\subsection{Bible Parsing}

We began by writing a parser for the bible data to turn it into a standardized format for our ngram model. The parser applied a few regular expressions: one to strip out the XML-like tags, one to replace psalm/verse numbers with the sentence deliminator (\texttt{<s>}), and one to add spaces around all punctuation. This resulted in a file with space around each token that could be used in our ngram model. 

\subsection{Hotel Review Parsing}

Hotel Reviews needed additional parsing unlike the Bible. For generating Random Sentences, our parse code removed the given numbers like IsTruthful and IsPositive. We used the Punkt sentence segmentation tool provided by nltk library. Punkt has a tokenizer that divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. We trained it on a large collection of plaintext in the target language before we used it. Punkt knows that the periods in Mr. Smith and Johann S. Bach do not mark sentence boundaries. And sometimes sentences can start with non-capitalized words. We made use of this sentence segmentation tool to recognize sentence boundaries.

{\small
\begin{verbatim}
sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')

data = open('raw_reviews.train', 'w')
    for c in sents:
        # adds sentence start and end marker
         # write to file
        data.write(c+'<s> ')
    data.close()
\end{verbatim}
}

After tokenizing with Punkt we wrote the text to another file by adding sentence markers denoted by $\langle$s$\rangle$ to appropriate sentence boundaries recognized by Punkt and any new line characters. We treated punctuation as separate words by adding spaces in between them but punctuation in between words like Wi-Fi and can't were not separated as separated but treated as part of the entire word. Abnormal punctuation that can be qualified as fillers like -- or ... were treated as separate words as well.

{\small
\begin{verbatim}
edit = re.sub('(</?(TEXT|DOC)>\n)|([0-9]+,[0-9],)|((\.+\s){2,})', '', edit)
edit = re.sub('\n|^', ' <s> ', edit)
edit = re.sub('([,!?();:"-&/$])', r' \1 ', edit)
edit = re.sub('(\.{1,})',r' \1 ', edit)
edit = re.sub('--', ' -- ', edit)
\end{verbatim}
}

For Hotel Review prediction another parser was implemented along the same lines. The code for this parsing is in \texttt{predictReview.py}. There are three steps to parsing the values: marking start and end of sentences using sentence segmentation tool like we did before, marking start and end of reviews, and separating true reviews from false ones. Firstly, the reviews were parsed through the sentence segmentation tool identifying the respective sentence boundaries. In addition, the beginning and end of a review was marked with $\langle$r$\rangle$ by replacing all new line characters with it right at the beginning as our training data distinguished one review from the other with new line character. The resultant text was stored as \texttt{parsed\_predictions.train}. Now for distinguishing the true reviews from false ones, we made separate files \texttt{false.train} - containing reviews whose IsTruthful value is 0 and \texttt{false.train} with reviews whose IsTruthful value is 1.\par

Using the start and end of review $\langle$r$\rangle$ token we split the entire text into a list. Each element of the list contains the text of one full review. 

{\small
\begin{verbatim}
# list of reviews
true_pos_list = tru.split('<r> <s> ')

For example, 
print true_pos_list[1]

0 , 0 , Swissotelchicago – I could go on and on about the food here .  <s> Their steak was cooked just the 
way I like it and I ............ outlet to charge it at night .  <s> It made using an alarm the next day a
little annoying… I’m not sure why my company recommended this place ;  but maybe I just got a bad room ?  <s>
\end{verbatim}
}
Once each review was extracted, the IsTruthful values were checked and based on its value each review was either written to \texttt{true.train} or \texttt{false.train}. After this, we inputted both true and false reviews as inidiviual corpii into our ngram model to build respective unigrams and bigrams. 

\section{Unsmoothed ngrams}

\subsection{unsmoothed ngram}

For our ngram model, we initially began by individually creating unigram and bigram classes, but decided that there was already much in common between the two systems, so an abstraction was not too hard to make. Hence we went immediately to an n-gram model. 

Our n-gram model is implemented as a class with a constructor that constructs the model. The constructor takes \texttt{sourceFile}, the source file for training the model, which must be made up of tokens separated by whitespace; n, the n-gram to create; \texttt{smooth} the type of smoothing to apply; \texttt{useUnk}, whether to create \texttt{<unk>} tokens in the corpus; and \texttt{direction}, the direction of ngram to create.

{\small\begin{verbatim}
    def __init__(self, sourceFile, n = 1, smooth = Smooth.NONE, useUnk = False, direction = Direction.LR):
\end{verbatim}}

Our n-gram constructor reads in the preprocessed sources, ignoring case, and splits the file into a list of tokens. We then pass over this list to generate the counts for each unigram or bigram using \texttt{\_initializeNgram()}. Counts are stored as a dictionary (which in python are  backed by hashtables) of dictionaries. Each entry in the first dictionary is a "row" of the table. The key in the outer dictionary is a tuple of the previous words. Thus for unigram there is only one entry in this dictionary, (), the value stored is another dictionary. This dictionary stores each word as the key and the value is its count in the corpus. For a bigram model, the outer dictionary stores the word that came first, for instance a key might be: ('the'). The corresponding dictionary value stores all the words that followed 'the' and the count for that specific bigram. It is important to note that each "row" (inner dictionary) of the table does not store counts of zero, so for instance if 'cow' never followed 'the' in the corpus, but 'cow' was in the corpus somewhere, there would be no entry for 'cow' in ('the')'s dictionary. We did this to save space as storing the entire count table would take up far too much memory. 

Our n-gram then calculates the corresponding probability table for the counts (using \texttt{\_generateProbabilities}). It does so by iterating over each row of the table and each entry in each row to calculate the appropriate conditional probabilities. In this case we must use Ordered Dictionaries (\texttt{OrderedDict} in python) instead of normal dictionaries such that an order is maintained which is important for random sentence generation. Once again we do not store probabilities that are zero in the table to save space. 

\subsubsection{Changes to support backoff}
Our implementation of generalized n-gram changed once we moved onto smoothing and onto our extensions. Instead of simply storing a single n-gram table we stored every single i-gram model where $i\in \lbrace 1,...,n \rbrace$. To do this instead of storing a single dictionary of dictionaries for counts we store a list of dictionaries of dictionaries. Thus each entry in the list is an i-gram, which is stored at index $i-1$. Each entry is the table of counts for that i-gram. We generate these tables in the same way as before, using a single pass over the corpus. We then did the same as before to generate the probability table for each i-gram and stored the tables in a list. As before we did not store entries in the probability and count tables that had a value of zero to save space.

\subsubsection{Changes to support smoothing}
Because smoothing by definition is meant to remove zeros that exist in the probability table, we could not implement smoothing while still storing entire probability tables due to the memory requirements of having every row be full in the table. Because of this we implemented a lazy probability row generator to replace the original system.

The new system always generates and stores the single row of the unigram table. We did this such that we could have easy access to unigram probabilities and also use it as an ordering for words such that the random sentence generator was always passing over a row in the same order. For the lazy row generator when a probability was asked for (using \texttt{getProbability(word,prev)}), the generator function (\texttt{\_getProbabilityRow(self, prev)})would iterate over the associated row of counts and generate the probability row with the appropriate smoothing. Smoothing was done using a smoothing function, this is explained in Section 3. We then cached rows (typically we cached up to about 2,500 rows) to better balance the memory-usage vs. time tradeoff. It is important to note that we still stored counts without storing zero counts because zeroes could be inferred by simply not existing in the appropriate i-gram's table.

\section{Random sentence generation}
To generate a random sentence from the ngram model, one calls \texttt{randomSentence()} from the model object. The function starts with a sentence segmentation marker.Using the appropriate number of previously generated words (based off what n-gram the model is, i.e. 0 for unigram, 1 for bigram) we get the correct probability row. We then walk the probability row using (\texttt{generateWord(row)}), which generates a random float between zero and one. \texttt{generateWord} walks the row, subtracting the probability of each entry from the random number we generated, until the value is below 0. We then select the word that caused the generated number to fall below 0 as the next word in the sentence. We repeat the process until we generate another sentence segmentation marker.

%TODO: Include examples here?%
\iffalse
\subsection{Examples}
\setlength{\parindent}{0cm}
\newcommand\npar{\par\smallskip}
Sample unigram sentences for the bible corpus:\npar
\texttt{<s> after ramah and , of . lord , shall this that every burnt land his my that the now all lord jabin's and few at flesh is waved he thou they of watered hath wife the unto gilead a stones might ; for ; the eliezer even above cherethites said the go of and ; that they family precious <s>}\npar
\texttt{<s> keep sin <s>}\npar
\texttt{<s> altar things ; and , : , three bags the which abraham : shall made his begat word saying snare we your i <s>}\npar
\texttt{<s> , <s>}\npar
\texttt{<s> children <s>}\npar
\bigskip

Sample bigram sentences for the bible corpus:\npar
\texttt{<s> for ashtoreth the son . <s>}\npar
\texttt{<s> david my father's house , the wheat and all the god hath sent messengers , from dophkah . <s>}\npar
\texttt{<s> and burned the land cast them , i will heal : make thee ? and the month was six hundred and the place . <s>}\npar
\texttt{<s> and thou hast found stealing any of the egyptians shall wash with him to eziongeber . <s>}\npar
\texttt{<s> beware . <s>}\npar
\bigskip

Sample unigram sentences for the hotel reviews corpus:\npar
\texttt{<s> i the ; linens . were able and our <s>}\npar
\texttt{<s> use from book fine at <s>}\npar
\texttt{<s> could said ranges smooth that course the phone expressed the its to minutes it because ; i'm we at it blamed ; <s>}\npar
\texttt{<s> faint coffee can the request were . chicago pillows fantastic intercontinental there . the the <s>}\npar
\texttt{<s> i've . mattress . at leisure a already centrally chicago don't an seem you they to the the or i booked great the find bothered time " . .... of . also above the on it . to explain my really the was online . will sub air ; good peace charge <s>}\npar
\bigskip

Sample bigram sentences for the hotel reviews corpus:\npar
\texttt{<s> we arrived at in my little lost and advertised as inside and we did inquire whether hard with this hotel in the services . <s>}\npar
\texttt{<s> we were not want or phone system " stains ; and could not a dirty and ready for business and the 10th floor ; and if you can hear they had earplugs . <s>}\npar
\texttt{<s> prior guest . <s>}\npar
\texttt{<s> there are typical - when reviewing my girlfriend are 21st century standards . <s>}\npar
\texttt{<s> the entire stay because its a room was pleased that can expect from the past the beautiful decor is saying that price . <s>}\npar

\fi

\section{Smoothing and unknown word handling}

Our n-gram model can perform Good-Turing smoothing. As mentioned in our ngram implementation, we are also able to specify the type of smoothing when the ngram class itself is constructed. Smoothing is defined as an ``enum'':

{\small\begin{verbatim}
    Smooth = enum('NONE', 'GOOD_TURING', 'ADD_ONE')
\end{verbatim}}

Smoothing is then specified when the ngram is constructed. (Smoothing is defaulted to \texttt{Smooth.NONE} when unspecified.) For example, the following code will generate an trigram representation for the text in \texttt{sourcefile}, and use Good-Turing smoothing:
{\small\begin{verbatim}
    ngram = ngram('sourcefile', 3, Smooth.GOOD_TURING)
\end{verbatim}}

Initially, we planned to generate new counts for each row depending on the smoothing method, but realized that this would use up an incredible amount of memory\ldots something which is undesirable. Our motivation to find a more memory-efficient solution led us to consider a functional approach.\par
For our n-gram model, our smoothing is implemented functionally: smoothing can simply be a function that takes in a count (of some n-gram and some $n$), and maps it to some other count depending on the smoothing method specified earlier in the constructor. (No-smoothing would an identity function. For add-one smoothing (one of our small extensions), the function simply spits out a count of 1 more than what's inputted.) For Good-Turing smoothing, we needed references to the frequency of frequencies (the $N_c$s) mentioned in lecture and the textbook before we can apply smoothing.\par

The following code computes these frequencies of frequencies, and selects the \emph{correct two-place smoothing function (that takes in the \emph{n} of the n-gram and an n-gram count)}.

{\small\begin{verbatim}
    self._populateNgramFreqs()
    self.smoothingFunction = self._smoothing()
\end{verbatim}}

\texttt{\_populateNgramFreqs} populates the $N$ table, a dictionary of count, and the number of n-grams that appear with a frequency of that count. The Good-Turing function (which references the frequency of frequency table) is defined as follows:

{\small\begin{verbatim}
    # function taking in ngram val (i), count nv, returns new cstar count
    def goodTuringFunction(i,nv):
        if nv < self.goodTuringLimit:
            cstar = (nv + 1.0) * self.ngramFreqs[i][nv+1] / self.ngramFreqs[i][nv]
            return cstar
        else:
            return nv
\end{verbatim}}

For example, if we wanted to find the Good-Turing smoothing value for a trigram of count 6: \texttt{goodTuringFunction(3,6)} would produce the correct new count.

We're not doing regression to substitute values for those without counts. For example, if we had positive counts for $N_1$, $N_2$, $N_3$, and $N_5$, but not $N_4$, we would want to do Good-Turing smoothing up to about 3 and stop. We did notice, however, that both corpuses have positive counts of unigrams, bigrams, and trigrams up to about 20, so it was reasonable to assume that Good-Turing smoothing can be done for n-gram frequencies less than 12. This threshold , 12, is in variable \texttt{self.goodTuringLimit}.

\subsection{Handling unknown words}
The n-gram constructor accepts a boolean \texttt{useUnk} (defaulted to False) that conditionally handles unknown words.\par
We are handling unknown words trivially, taking a suggestion from the Jurafsky textbook, where we replace the first instance of each word in the corpus with the unknown token ``\texttt{<unk>}''. We then proceed with processing and generating counts in the same was as before.

When finding the probability of a given word, and previous words to consider. If the word was not in the vocabulary of the test set, or words in the previous words were not in the test set they were converted to \texttt{<unk>} so that the appropriate probability could be found.

\section{Perplexity}

To implement perplexity we added a function \texttt{perplexity(test)} that takes in the file name of the test corpus (must be made up of tokens separated by whitespace). The function then reads the test corpus into a list and iterates over each "window" of the corpus, where a window is each set of $n$ words in a sequence. For example for a bigram model with a test corpus of "a cow went moo" it would iterate over "a cow", "cow went" and "went moo". Then for each window it finds the probability of the last word of the window, given the previous words. It then adds $log\left(\frac{1}{P(w_i|w_{i-1},...,w_{i-n+1})}\right)$ to a running total by doing: \texttt{pp += math.log(1/self.getProbability(word, prev))}. After going over the entire corpus the function then multiplies \texttt{pp} by $\frac{1}{N}$, e.g. \texttt{pp *= (1.0/len(test\_corpus))}. Finally the perplexity is obtained by raising $e$ to the power \texttt{pp}, e.g. \texttt{pp = math.exp(pp)}.

We use log rules to avoid overflow during the perplexity computation. Perplexity is defined as:\linebreak $PP=\left(\mathlarger{\prod}_{i=0}^N\frac{1}{P(w_i|w_{i-1},...,w_{i-n+1})}\right)^{1/N}$. By using log rules we can re-write this as: \linebreak $log(PP) = \log\left(\mathlarger{\prod}_{i=0}^N\frac{1}{P(w_i|w_{i-1},...,w_{i-n+1})}\right)^{1/N} = \frac{1}{N}\mathlarger{\sum}_{i=0}^N log\left(\frac{1}{P(w_i|w_{i-1},...,w_{i-n+1})}\right)$. We can then simply recover $PP$ by doing $e^{log(PP)}$. This is exactly what we perform in our perplexity function as explained in the preceding paragraph.

\section{Hotel review truthfulness predictions}
Our code for this part of the project is in \texttt{predictReview.py}.
\lipsum[4]

\section{Extensions}

\subsection{Generalized n-gram}

The description of our generalized n-gram model is in Section 1, while necessary changes for n-grams where $n\geq3$ that caused a need for backoff are described in the following section.


\subsection{Backoff)

Backoff is necessary when one gets to $n \geq 3$ because at that point trigrams (or any n-gram where $n \geq 3$) may appear for which we do not have a row in our lazily generated probability table. By this I mean that the 'a green cow' may be passed to our \texttt{getProbability()} function, e.g. we may be asked the probability of 'cow' given 'a green'. In this case we have seen 'a' and 'green' and 'cow' in our corpus and thus are in our vocabulary, but we have never seen 'a green', thus there is no row in our trigram table for 'a green'. In this case backoff is appropriate. 

To implement backoff we made \texttt{getProbability()} recursive. If the row for the previous words does not exist in our table, then we remove the oldest of the previous words and try with the remaining $i-1$ words, we do this until a row is found. This is the exact reason why when creating our generalized n-gram we decided to store all i-grams where $i \in \lbrace 1,...,n \rbrace$. Our backoff is relatively naive as it does not backoff in the case of low probabilities, instead backing off only if absolutely necessary.

\subsection{Add-One Smoothing} %TODO: COMPLETE%

\subsection{Right-to-Left n-grams}

In order to implement n-grams that used the right-to-left ordering of words instead of left-to-right we had to change a few things. First to clarify: right-to-left means if the trigram 'a cow moos' was in the training corpus, 'a' would be stored with a count in the row for 'moos cow' instead of 'moos' being stored in the row for 'a cow'. The first thing to do was after pulling the training corpus into the list, we completely reversed the list. After that processing to generate the model could occur as before. We also had to change our perplexity and randomSentence functions slightly. Our perplexity function has an added conditional that reverses the test corpus if the model is right-to-left. Our randomSentence function runs the same way as before, but if the model is right-to-left the final output of the sentence has to be reversed.

\section{Experimentation}
%TODO: COMPLETE%

Bible.train and bible.test perplexity scores 
Good Turing: 159.791135281 (bigram)
Good Turing: 158.595972171 (bigram, RL)
Add One: 280.215776754 (bigram)
Good Turing smoothing: 243.432299431 (unigram)
Good Turing smoothing: 243.433400061 (trigram)

\section{Division of work}
We divided up our tasks as follows. While we did split up responsibilities, each of us contributed well to all other aspects of the programming and writeup.

\begin{itemize}[noitemsep]
\item Ben Shulman:
  \begin{itemize}[noitemsep,nolistsep]
  \item Generalized n-gram model, Backoff, Right-to-left n-grams and random sentence generation
  \item Perplexity
  \end{itemize}
\item Spandana Govindgari:
  \begin{itemize}[noitemsep,nolistsep]
  \item Parsing Hotel Reviews
  \item Hotel Reviews truthfulness prediction
  \end{itemize}
\item Andy Wang:
  \begin{itemize}[noitemsep,nolistsep]
  \item Good-Turing smoothing and add-one smoothing
  \item Handling unknown words
  \end{itemize}
\end{itemize}

\section{How to run the code}
Our three python scripts are \texttt{preprocessCorpus.py}, \texttt{ngram.py}, and \texttt{predictReview.py}.\par
Running \texttt{preprocessCorpus} will\ldots\par
Running \texttt{ngram} will\ldots\par
Running \texttt{predictReview} will\ldots\par

Our output files\ldots

\lipsum[4]

\section{Other comments}
Thanks!

\end{document}
