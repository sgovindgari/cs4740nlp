%!TEX TS-program = pdflatex or xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass{article}
\usepackage[letterpaper, margin=1in, left=0.75in, right=0.75in] {geometry}
\usepackage{microtype} % micro appearance
\usepackage[all]{nowidow} % no lone lines
\usepackage{changepage} % changes layout mid-page
\usepackage{enumitem} % enum (itemize, enumerate, description)
\usepackage{ifthen}
\usepackage{xfrac}
\usepackage{fancyhdr} % headers
\usepackage{amsmath} % matrices
\usepackage{amssymb} % symbols
\usepackage{gensymb} % symbols
\usepackage{tikz} % graphics?
\usepackage{bm} % bold math
\usepackage{multicol} % multiple columns
\usepackage{lipsum} % lipsum

\pagestyle{fancy}

\newcommand {\classname} {CS 4740/5740 -- Intro to Natural Language Processing}
\newcommand {\duedate} {Mon, February 24}
\newcommand {\hwtype} {Project}
\newcommand {\hwnum} {1: Language Modeling}
\newcommand {\innermargin} {0.15in} % indentation amt of nested lst

\let\tss\textsuperscript
\let\oldunderline\underline
\renewcommand\underline[1]{\oldunderline{\smash{#1}}}

\fancyhead[L] {\ifthenelse {\thepage=1}
  {bgs53, sg754, amw275\\
   Ben Shulman, Spandana Govindgari, Andy Wang}
  {\hwtype\ \hwnum\ (bgs53, sg754, amw275)}}
\fancyhead[R] {\ifthenelse {\thepage=1}
  {\classname\\\hwtype\ \hwnum\ (due \duedate)}
  {Page \thepage}}

\begin{document}
\begin{center}\textbf{Project 1 Report on Language Modeling}\end{center}

All our code is written in Python. We use several libraries, including the nltk platform's punkt for preprocessing.\par

\section{Unsmoothed ngrams}

\subsection{Bible Parsing}

We began by writing a parser for 

\subsection{Hotel Review Parsing}

\subsection{unsmoothed ngram}

Our preprocessing code for both corpuses (King James Bible and Hotel Reviews) is in \texttt{preprocessCorpus.py}.\par

Before we inputted the training data for both corpuses into our ngram implementation, we wrote a script to preprocess them, removing extraneous xml-like tags, numbers, etc. For the bible corpus, our sentences are split on Bible verses of the format \texttt{[0-9]+:[0-9]+}. Ben and Spandana has written much of the preprocessor for the Bible and Hotel corpuses respectively.\par

For our ngram model, we initially began by individually creating unigram and bigram classes, but decided that there was already much in common between the two systems, so an abstraction was not too hard to make. Hence we went immediately to an n-gram model. Our ngram class reads in the preprocessed sources, ignores case, and splits the text at spaces.\par

Our storage is a list of dictionaries, each of which store unigram/bigram/trigram/etc data. The dictionaries record counts of the $p$ previous words followed by a word, i.e.\ for a bigram model, it stores the unigram counts and bigram counts. Each dictionary then holds an entry (another lookup table) for each tuple of previous words, i.e.\ for unigram there is only one entry: \texttt{[((), [(the,5),(a,6),(cat,2),...])]}. For bigram, there would be \texttt{[('the',[('cat', 3),('dog', 4),...]),('a',[(cow, 2),(horse, 1),...]),...]}\par

Our ngram steps through the corpus word-by-word, and accumulate a list of previous words (no longer than $n$) to use as a lookup. After this has been set, an optional smoothing algorithm (like add-one, or Good-Turing) follows. Probabilities are calculated last by going over every row of counts and calculating the conditional probabilities for each row.\par

[talk about responsibilities and files here]\par
[talk about the order in which unk, smoothing, right-to-left is done, etc]

\subsection{Hotel reviews}
For Hotel reviews we used the Punkt sentence segmentation tool provided by nltk library. Punkt has a tokenizer that divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. We trained it on a large collection of plaintext in the target language before we used it. Punkt knows that the periods in Mr. Smith and Johann S. Bach do not mark sentence boundaries. And sometimes sentences can start with non-capitalized words. We made use of this sentence segmentation tool to recognize sentence boundaries. 

\section{Random sentence generation}
To generate a random sentence we start with a sentence start marker. We then generate a random float (0 to 1)
and then simply traverse the appropriate row of our probability table until we reach the appropriate accumulated probability and output that word.\par

We continue to generate words until we generate another sentence segmentation marker.

[Include examples of generator in action! Possibly put the following code (below) as an appendix?]

\iffalse
\vspace{2mm}
\setlength{\parindent}{0cm}
\newcommand\npar{\par\smallskip}
Sample unigram sentences for the bible corpus:\npar
\texttt{<s> after ramah and , of . lord , shall this that every burnt land his my that the now all lord jabin's and few at flesh is waved he thou they of watered hath wife the unto gilead a stones might ; for ; the eliezer even above cherethites said the go of and ; that they family precious <s>}\npar
\texttt{<s> keep sin <s>}\npar
\texttt{<s> altar things ; and , : , three bags the which abraham : shall made his begat word saying snare we your i <s>}\npar
\texttt{<s> , <s>}\npar
\texttt{<s> children <s>}\npar
\bigskip

Sample bigram sentences for the bible corpus:\npar
\texttt{<s> for ashtoreth the son . <s>}\npar
\texttt{<s> david my father's house , the wheat and all the god hath sent messengers , from dophkah . <s>}\npar
\texttt{<s> and burned the land cast them , i will heal : make thee ? and the month was six hundred and the place . <s>}\npar
\texttt{<s> and thou hast found stealing any of the egyptians shall wash with him to eziongeber . <s>}\npar
\texttt{<s> beware . <s>}\npar
\bigskip

Sample unigram sentences for the hotel reviews corpus:\npar
\texttt{<s> i the ; linens . were able and our <s>}\npar
\texttt{<s> use from book fine at <s>}\npar
\texttt{<s> could said ranges smooth that course the phone expressed the its to minutes it because ; i'm we at it blamed ; <s>}\npar
\texttt{<s> faint coffee can the request were . chicago pillows fantastic intercontinental there . the the <s>}\npar
\texttt{<s> i've . mattress . at leisure a already centrally chicago don't an seem you they to the the or i booked great the find bothered time " . .... of . also above the on it . to explain my really the was online . will sub air ; good peace charge <s>}\npar
\bigskip

Sample bigram sentences for the hotel reviews corpus:\npar
\texttt{<s> we arrived at in my little lost and advertised as inside and we did inquire whether hard with this hotel in the services . <s>}\npar
\texttt{<s> we were not want or phone system " stains ; and could not a dirty and ready for business and the 10th floor ; and if you can hear they had earplugs . <s>}\npar
\texttt{<s> prior guest . <s>}\npar
\texttt{<s> there are typical - when reviewing my girlfriend are 21st century standards . <s>}\npar
\texttt{<s> the entire stay because its a room was pleased that can expect from the past the beautiful decor is saying that price . <s>}\npar

\fi

\section{Smoothing and unknown word handling}

Our n-gram model can perform Good-Turing smoothing. As mentioned in our ngram implementation, we are also able to specify the type of smoothing when the ngram class itself is constructed. Smoothing is defined as an ``enum'':

{\small\begin{verbatim}
    Smooth = enum('NONE', 'GOOD_TURING', 'ADD_ONE')
\end{verbatim}}

Smoothing is then specified when the ngram is constructed. (Smoothing is defaulted to \texttt{Smooth.NONE} when unspecified.) For example, the following code will generate an trigram representation for the text in \texttt{sourcefile}, and use Good-Turing smoothing:
{\small\begin{verbatim}
    ngram = ngram('sourcefile', 3, Smooth.GOOD_TURING)
\end{verbatim}}

Initially, we planned to generate new counts for each row depending on the smoothing method, but realized that this would use up an incredible amount of more memory\ldots something perhaps undesirable. Our motivation to find a more memory-efficient solution led us to consider a functional approach.\par
For our n-gram model, our smoothing is implemented functionally: smoothing can simply be a function that takes in a count (of some n-gram and some $n$), and maps it to some other count depending on the smoothing method specified earlier in the constructor. (No-smoothing would an identity function. For add-one smoothing (one of our small extensions), the function simply spits out a count of 1 more than what's inputted.) For Good-Turing smoothing, we needed references to the frequency of frequencies (the $N_c$s) mentioned in lecture and the textbook before we can apply smoothing.\par

The following code computes these frequencies of frequencies, and selects the \emph{correct two-place smoothing function (that takes in the \emph{n} of the n-gram and an n-gram count)}.

{\small\begin{verbatim}
    self._populateNgramFreqs()
    self.smoothingFunction = self._smoothing()
\end{verbatim}}

\texttt{\_populateNgramFreqs} populates the $N$ table, a dictionary of count, and the number of n-grams that appear with a frequency of that count. The Good-Turing function (which references the frequency of frequency table) is defined as follows:

{\small\begin{verbatim}
    # function taking in ngram val (i), count nv, returns new cstar count
    def goodTuringFunction(i,nv):
        if nv < self.goodTuringLimit:
            cstar = (nv + 1.0) * self.ngramFreqs[i][nv+1] / self.ngramFreqs[i][nv]
            return cstar
        else:
            return nv
\end{verbatim}}

For example, if we wanted to find the Good-Turing smoothing value for a trigram of count 6: \texttt{goodTuringFunction(3,6)} would produce the correct new count.

We're not doing regression to substitute values for those without counts. For example, if we had positive counts for $N_1$, $N_2$, $N_3$, and $N_5$, but not $N_4$, we would want to do Good-Turing smoothing up to about 3 and stop. We did notice, however, that both corpuses have positive counts of unigrams, bigrams, and trigrams up to about 20, so it was reasonable to assume that Good-Turing smoothing can be done for n-gram frequencies less than 12. This threshold , 12, is in variable \texttt{self.goodTuringLimit}.

\subsection{Handling unknown words}
The n-gram constructor accepts a boolean \texttt{useUnk} (defaulted to False) that conditionally handles unknown words.\par
We are handling unknown words trivially, taking a suggestion from the Jurafsky textbook, where we replace the first instance of some word we see with the unknown token ``\texttt{<unk>}''. Hence, probabilities for unknowns are calculated as always, and the counts of every n-gram is reduced by 1.

\section{Perplexity}
\lipsum[4]

\section{Hotel review truthfulness predictions}
Our code for this part of the project is in \texttt{predictReview.py}.
\lipsum[4]

\section{Extensions}

\subsection{Generalized n-gram}

\subsection{Backoff}

\subsection{Add-One Smoothing}

\subsection{Right-to-Left n-grams}

\section{Further extensions and possibilities}
Can you perform any experiments that support the need for extension of change? (trigram backing off?)\par
What experiments to show whether or not your extension has desired effect?\par
\lipsum[4]

\section{Division of work}
We divided up our tasks as follows. While we did split up responsibilities, each of us contributed well to all other aspects of the programming and writeup.

\begin{itemize}[noitemsep]
\item Ben Shulman:
  \begin{itemize}[noitemsep,nolistsep]
  \item Generalized n-gram model and random sentence generation
  \item Perplexity
  \end{itemize}
\item Spandana Govindgari:
  \begin{itemize}[noitemsep,nolistsep]
  \item Parsing Hotel Reviews
  \item Hotel Reviews truthfulness prediction
  \end{itemize}
\item Andy Wang:
  \begin{itemize}[noitemsep,nolistsep]
  \item Good-Turing smoothing and add-one smoothing
  \item Handling unknown words
  \end{itemize}
\end{itemize}

\section{How to run the code}
Our three python scripts are \texttt{preprocessCorpus.py}, \texttt{ngram.py}, and \texttt{predictReview.py}.\par
Running \texttt{preprocessCorpus} will\ldots\par
Running \texttt{ngram} will\ldots\par
Running \texttt{predictReview} will\ldots\par

Our output files\ldots

\lipsum[4]

\section{Other comments}
Thanks!

\end{document}
