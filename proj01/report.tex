%!TEX TS-program = pdflatex or xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass{article}
\usepackage[letterpaper, margin=1in, left=0.75in, right=0.75in] {geometry}
\usepackage{microtype} % micro appearance
\usepackage[all]{nowidow} % no lone lines
\usepackage{changepage} % changes layout mid-page
\usepackage{enumitem} % enum (itemize, enumerate, description)
\usepackage{ifthen}
\usepackage{xfrac}
\usepackage{fancyhdr} % headers
\usepackage{amsmath} % matrices
\usepackage{amssymb} % symbols
\usepackage{relsize}
\usepackage{gensymb} % symbols
\usepackage{tikz} % graphics?
\usepackage{bm} % bold math
\usepackage{multicol} % multiple columns
\usepackage{lipsum} % lipsum

\pagestyle{fancy}

\newcommand {\classname} {CS 4740/5740 -- Intro to Natural Language Processing}
\newcommand {\duedate} {Mon, February 24}
\newcommand {\hwtype} {Project}
\newcommand {\hwnum} {1: Language Modeling}
\newcommand {\innermargin} {0.15in} % indentation amt of nested lst

\let\tss\textsuperscript
\let\oldunderline\underline
\renewcommand\underline[1]{\oldunderline{\smash{#1}}}

\fancyhead[L] {\ifthenelse {\thepage=1}
  {bgs53, sg754, amw275\\
   Ben Shulman, Spandana Govindgari, Andy Wang}
  {\hwtype\ \hwnum\ (bgs53, sg754, amw275)}}
\fancyhead[R] {\ifthenelse {\thepage=1}
  {\classname\\\hwtype\ \hwnum\ (due \duedate)}
  {Page \thepage}}

\begin{document}
\begin{center}\textbf{Project 1: Report on Language Modeling}\end{center}

\section*{Introduction}

For this project we have written our code in Python. We used several of pythons included libraries and nltk's punkt for sentence segmentation of the hotel reviews data. To run our code, please reference our README.

\section{Parsing text}
Our preprocessing code for both corpuses (King James Bible and Hotel Reviews) is in \texttt{preprocessCorpus.py}.\par

Before we inputted the training data for both corpuses into our ngram implementation, we wrote a script to preprocess them, removing extraneous xml-like tags, numbers, etc. Ben and Spandana have contributed to the preprocessor for the Bible and Hotel corpuses respectively.\par

\subsection{Bible Parsing}

We began by writing a parser for the bible data to turn it into a standardized format for our ngram model. The parser applied a few regular expressions: one to strip out the XML-like tags, one to replace psalm/verse numbers with the sentence deliminator (\texttt{<s>}), and one to add spaces around all punctuation. This resulted in a file with space around each token that could be used in our ngram model. 

\subsection{Hotel Review Parsing}

Hotel Reviews needed additional considerations. For generating Random Sentences, our parse code removed the given numbers like IsTruthful and IsPositive. Punkt's tokenizer (nltk library) divides a text into a list of sentences with an unsupervised algorithm. We trained it on a large collection of plaintext in the target language before we used it.

{\small
\begin{verbatim}
sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
data = open('raw_reviews.train', 'w')
    for c in sents:
        # adds sentence start and end marker
         # write to file
        data.write(c+'<s> ')
    data.close()
\end{verbatim}
}

After the Punkt tokenization, we added sentence markers (\texttt{<s>}) to boundaries recognized by Punkt and any new line characters. We treated punctuation as separate words by adding spaces in between them but punctuation in between words like Wi-Fi and can't were not separated as separated but treated as part of the entire word.

For Hotel Review \emph{prediction}, another parser was implemented along the same lines. The code for this parsing is in \texttt{predictReview.py}. There are three steps to parsing the values: marking start and end of sentences using sentence segmentation tool like we did before, marking start and end of reviews (with the token \texttt{<r>}), and separating true reviews from false ones. The resultant text was stored as \texttt{parsed\_predictions.train}. For distinguishing the true reviews from false ones, we made separate files \texttt{false.train} - containing reviews whose IsTruthful value is 0 and \texttt{false.train} with reviews whose IsTruthful value is 1. The reviews are split into a list:\par

{\small\begin{verbatim}
# list of reviews
true_pos_list = tru.split('<r> <s> ')

print true_pos_list[1]

0 , 0 , Swissotelchicago – I could go on and on about the food here .  <s> Their steak was cooked just the 
way I like it and I ............ outlet to charge it at night .  <s> It made using an alarm the next day a
little annoying… I’m not sure why my company recommended this place ;  but maybe I just got a bad room ?  <s>
\end{verbatim}
}
Once each review was extracted, the IsTruthful values were checked and based on its value each review was either written to \texttt{true.train} or \texttt{false.train}. After this, we inputted both true and false reviews as inidiviual corpii into our ngram model to build respective n-grams. (At one point, we used left-to-right and right-to-left unigram, bigram, trigram, 4-grams to attempt to get a better score.)

\section{Unsmoothed ngrams}

\subsection{Unsmoothed ngram}

For our ngram model, we initially began by individually creating unigram and bigram classes, but decided that there was already much in common between the two systems, so an abstraction was not too hard to make. Hence we went immediately to an n-gram model. 

Our n-gram model is implemented as a class with a constructor that constructs the model. The constructor takes \texttt{sourceFile}, the source file for training the model, which must be made up of tokens separated by whitespace; n, the n-gram to create; \texttt{smooth}, the type of smoothing to apply; \texttt{useUnk}, whether to handle unknowns by creating \texttt{<unk>} tokens in the corpus; and \texttt{direction}, the direction of ngram to create. (Direction is one of our extensions, discussed on more detail later.)

{\small\begin{verbatim}
    def __init__(self, sourceFile, n = 1, smooth = Smooth.NONE, useUnk = False, direction = Direction.LR):
\end{verbatim}}

Our n-gram constructor reads in the preprocessed sources, ignoring case, and splits the file into a list of tokens. We then pass over this list to generate the counts for each unigram or bigram using \texttt{\_initializeNgram()}. Counts are stored as a dictionary (which in python are  backed by hashtables) of dictionaries. Each entry in the first dictionary is a ``row'' of the table. The key in the outer dictionary is a tuple of the previous words. Thus for unigram there is only one entry in this dictionary, \texttt{()}, the value stored is another dictionary. This dictionary stores each word as the key and the value is its count in the corpus. For a bigram model, the outer dictionary stores the word that came first, for instance a key might be: \texttt{(`the')}. The corresponding dictionary value stores all the words that followed `the' and the count for that specific bigram. It is important to note that each ``row'' (inner dictionary) of the table does not store counts of zero, so for instance if `cow' never followed `the' in the corpus, but `cow' was in the corpus somewhere, there would be no entry for `cow' in \texttt{(`the')}'s dictionary. We did this to save space as storing the entire count table would take up far too much memory.\par
Here is an example bigram count table for the corpus ``a dog and a cat and a cat'':
\begin{verbatim}
{   ('a',): {'cat': 2, 'dog': 1},
    ('and',): {'a': 2},
    ('cat',): {'and': 1},
    ('dog',): {'and': 1}
}
\end{verbatim}

Our n-gram then calculates the corresponding probability table for the counts (using \texttt{\_generateProbabilities}). It does so by iterating over each row of the table and each entry in each row to calculate the appropriate conditional probabilities. In this case we must use Ordered Dictionaries (\texttt{OrderedDict} in python) instead of normal dictionaries such that an order is maintained which is important for random sentence generation. Once again we do not store probabilities that are zero in the table to save space. 

\subsubsection{Changes to support backoff}
Our implementation of generalized n-gram changed once we moved onto smoothing and onto our extensions. Instead of simply storing a single n-gram table we stored every single i-gram model where $i\in \lbrace 1,...,n \rbrace$. To do this instead of storing a single dictionary of dictionaries for counts we store a list of dictionaries of dictionaries. Thus each entry in the list is an i-gram, which is stored at index $i-1$. Each entry is the table of counts for that i-gram. We generate these tables in the same way as before, using a single pass over the corpus. We then did the same as before to generate the probability table for each i-gram and stored the tables in a list. As before we did not store entries in the probability/count tables that had value zero.

\subsubsection{Changes to support smoothing}
Because smoothing by definition is meant to remove zeros that exist in the probability table, we could not implement smoothing while still storing entire probability tables due to the memory requirements of having every row be full in the table. Because of this we implemented a lazy probability row generator to replace the original system.

The new system always generates and stores the single row of the unigram table. We did this such that we could have easy access to unigram probabilities and also use it as an ordering for words such that the random sentence generator was always passing over a row in the same order. For the lazy row generator when a probability was asked for (using \texttt{getProbability(word,prev)}), the generator function (\texttt{\_getProbabilityRow(self, prev)})would iterate over the associated row of counts and generate the probability row with the appropriate smoothing. We do this by generating the new smoothed counts for the row and then normalize them. Smoothing was done using a smoothing function, this is explained in Section 4. We then cached rows (typically we cached up to about 2,500 rows) to better balance the memory-usage vs. time tradeoff. We still do not store zero counts in the count tables as they can be inferred.

\section{Random sentence generation}
To generate a random sentence from the ngram model, one calls \texttt{randomSentence()} from the model object. The function starts with a sentence segmentation marker. Using the appropriate number of previously generated words (based off what n-gram the model is, i.e.\ 0 for unigram, 1 for bigram) we get the correct probability row. We then walk the probability row using (\texttt{generateWord(row)}), which generates a random float between zero and one. \texttt{generateWord} walks the row, subtracting the probability of each entry from the random number we generated, until the value is below 0. We then select the word that caused the generated number to fall below 0 as the next word in the sentence. We repeat the process until we generate another sentence segmentation marker.

\subsection{Examples}

\begin{adjustwidth}{0.5in}{0.5in}
\setlength{\parindent}{-0.25in}
\newcommand\npar{\par}
Sample unigram sentences for the bible corpus:\npar
{\footnotesize
\texttt{<s> after ramah and , of . lord , shall this that every burnt land his my that the now all lord jabin's and few at flesh is waved he thou they of watered hath wife the unto gilead a stones might ; for ; the eliezer even above cherethites said the go of and ; that they family precious <s>}\npar
\texttt{<s> keep sin <s>}\npar
\texttt{<s> altar things ; and , : , three bags the which abraham : shall made his begat word saying snare we your i <s>}\npar
}\bigskip

Sample bigram sentences for the bible corpus:\npar
{\footnotesize
\texttt{<s> for ashtoreth the son . <s>}\npar
\texttt{<s> david my father's house , the wheat and all the god hath sent messengers , from dophkah . <s>}\npar
\texttt{<s> and thou hast found stealing any of the egyptians shall wash with him to eziongeber . <s>}\npar
}\bigskip

Sample unigram sentences for the hotel reviews corpus:\npar
{\footnotesize
\texttt{<s> i the ; linens . were able and our <s>}\npar
\texttt{<s> could said ranges smooth that course the phone expressed the its to minutes it because ; i'm we at it blamed ; <s>}\npar
\texttt{<s> faint coffee can the request were . chicago pillows fantastic intercontinental there . the the <s>}\npar
}\bigskip

Sample bigram sentences for the hotel reviews corpus:\npar
{\footnotesize
\texttt{<s> we arrived at in my little lost and advertised as inside and we did inquire whether hard with this hotel in the services . <s>}\npar
\texttt{<s> prior guest . <s>}\npar
\texttt{<s> the entire stay because its a room was pleased that can expect from the past the beautiful decor is saying that price . <s>}\npar
}\end{adjustwidth}

\section{Smoothing and unknown word handling}
Our n-gram model can perform Good-Turing smoothing. As mentioned in our ngram implementation, we are also able to specify the type of smoothing when the ngram class itself is constructed. Smoothing is defined as an ``enum'':

{\small\begin{verbatim}
    Smooth = enum('NONE', 'GOOD_TURING', 'ADD_ONE')
\end{verbatim}}

Smoothing is then specified when the ngram is constructed. (Smoothing is defaulted to \texttt{Smooth.NONE} when unspecified.) For example, the following code will generate an trigram representation for the text in \texttt{sourcefile}, and use Good-Turing smoothing:
{\small\begin{verbatim}
    ngram = ngram('sourcefile', 3, Smooth.GOOD_TURING)
\end{verbatim}}

Initially, we planned to generate new counts for each row depending on the smoothing method, but realized that this would use up an incredible amount of memory. Our motivation to find a more memory-efficient solution led us to consider a functional approach.\par
For our n-gram model, our smoothing is implemented functionally: smoothing can simply be a function that takes in a count (of some n-gram and some $n$), and maps it to some other count depending on the smoothing method specified earlier in the constructor. (No-smoothing would an identity function. For add-one smoothing (one of our small extensions), the function simply spits out a count of 1 more than what's inputted.) For Good-Turing smoothing, we needed references to the frequency of frequencies (the $N_c$s) mentioned in lecture and the textbook before we can apply smoothing.\par

The following code computes these frequencies of frequencies, and selects the \emph{correct two-place smoothing function (that takes in the \emph{n} of the n-gram and an n-gram count)}.

{\small\begin{verbatim}
    self._populateNgramFreqs()
    self.smoothingFunction = self._smoothing()
\end{verbatim}}

\texttt{\_populateNgramFreqs} populates the $N$ table, a dictionary of count, and the number of n-grams that appear with a frequency of that count. The Good-Turing function (which references the frequency of frequency table) is defined as follows:

{\small\begin{verbatim}
    # function taking in ngram val (i), count nv, returns new cstar count
    def goodTuringFunction(i,nv):
        if nv < self.goodTuringLimit:
            cstar = (nv + 1.0) * self.ngramFreqs[i][nv+1] / self.ngramFreqs[i][nv]
            return cstar
        else:
            return nv
\end{verbatim}}

For example, if we wanted to find the Good-Turing smoothing value for a trigram of count 6:\par\texttt{goodTuringFunction(3,6)} would produce the correct new count.

We're not doing regression to substitute values for those without counts. For example, if we had positive counts for $N_1$, $N_2$, $N_3$, and $N_5$, but not $N_4$, we would want to do Good-Turing smoothing up to about 3 and stop. We did notice, however, that both corpuses have positive counts of unigrams, bigrams, and trigrams up to about 20, so it was reasonable to assume that Good-Turing smoothing can be done for n-gram frequencies less than 12. This threshold, 12, is in variable \texttt{self.goodTuringLimit}.

\subsection{Handling unknown words}
The n-gram constructor accepts a boolean \texttt{useUnk} (defaulted to False) that conditionally handles unknown words.\par
We are handling unknown words trivially, taking a suggestion from the Jurafsky textbook, where we replace the first instance of each word in the corpus with the unknown token ``\texttt{<unk>}''. We then proceed with processing and generating counts in the same was as before.

When finding the probability of a given word, and previous words to consider, if the word was not in the vocabulary of the test set, or words in the previous words were not in the test set they were converted to \texttt{<unk>} so that the appropriate probability could be found.

\section{Perplexity}

To implement perplexity we added a function \texttt{perplexity(test)} that takes in the file name of the test corpus (must be made up of tokens separated by whitespace). The function then reads the test corpus into a list and iterates over each ``window'' of the corpus, where a window is each set of $n$ words in a sequence. For example for a bigram model with a test corpus of ``a cow went moo'' it would iterate over ``a cow'', ``cow went'' and ``went moo''. Then for each window it finds the probability of the last word of the window, given the previous words. It then adds $\log\left(\dfrac{1}{P(w_i|w_{i-1},...,w_{i-n+1})}\right)$ to a running total by doing: \texttt{pp += math.log(1/self.getProbability(word, prev))}. After going over the entire corpus the function then multiplies \texttt{pp} by $\frac{1}{N}$, e.g. \texttt{pp *= (1.0/len(test\_corpus))}. Finally the perplexity is obtained by raising $e$ to the power \texttt{pp}, e.g. \texttt{pp = math.exp(pp)}.

We use log rules to avoid overflow during the perplexity computation.
Perplexity is defined as:
\[PP=\left(\prod_{i=0}^N\frac{1}{P(w_i|w_{i-1},...,w_{i-n+1})}\right)^{1/N}.\]
By using log rules we can re-write this as:
\[log(PP) = \log\left(\prod_{i=0}^N\frac{1}{P(w_i|w_{i-1},...,w_{i-n+1})}\right)^{1/N} = \frac{1}{N}\sum_{i=0}^N \log\left(\frac{1}{P(w_i|w_{i-1},...,w_{i-n+1})}\right).\] We can then simply recover $PP$ by doing $e^{log(PP)}$. This is exactly what we perform in our perplexity function as explained in the preceding paragraph, all of these logs (in our code as well) are base $e$.

\section{Hotel review truthfulness predictions} %TODO: COMPLETE
Our code for this part of the project is in \texttt{predictReview.py} under the method \texttt{predictReview()}. Our prediction technique uses the concept of perplexity. Basically, we train the reviews under unigram and bigram models with Good Turing smoothing and unknowns for both true as well as false training data. The test data is then used against these unigram and bigram models (tru\_unigram, fal\_unigram, tru\_bigram and fal\_bigram) to calculate for the perplexity. The lesser the perplexity the more likely that the review belongs to that particular model.\par

In order to determine the perplexities, we first parse the test data using our parser to create a text file called \texttt{predictions.test}. This file contains the parsed test data which is then split according to the start and end of review token $\langle$r$\rangle$ to create a list of reviews. We then use the unigram and bigram models generated for true and false training data to calculate the perplexity for each review. Since our perplexity function inputs a corpus, each review was written to a temporary text file and extracted which could have hurt our performance. The calculated values are then compared to find the minimum to determine the model that this review is closest to. Depending on if it is either true unigram or true bigram then the review is predicted to be true otherwise false. The value is written to a separate file along with the review called \texttt{final\_predictions.txt}.\par

For example, for the first test review our values for the models were as follows:
{\small\begin{verbatim}
    true unigram pp = 287.6
    true bigram pp  = 977.1
    false unigram pp= 247.6
    false bigram pp = 435.6
\end{verbatim}}

Since the false unigram model is less perplexed to see the text in the review we predict that the review is false as it contains words that are used in most of the false reviews and hence write a 0 as IsTruthful value for this review.

On Kaggle, we are ``The ABS''.

\section{Extensions}

\subsection{Generalized n-gram}
The description of our generalized n-gram model is in Section 1, while necessary changes for n-grams where $n\geq3$ that caused a need for backoff are described in the following section. We have example sentences produced by trigram and 4-gram in sections below.

\subsection{Backoff}
Backoff is necessary when one gets to $n \geq 3$ because at that point trigrams (or any n-gram where $n \geq 3$) may appear for which we do not have a row in our lazily generated probability table. By this I mean that the `a green cow' may be passed to our \texttt{getProbability()} function, e.g. we may be asked the probability of `cow' given `a green'. In this case we have seen `a' and `green' and `cow' in our corpus and thus are in our vocabulary, but we have never seen `a green', thus there is no row in our trigram table for `a green'. In this case backoff is appropriate. 

To implement backoff we made \texttt{getProbability()} recursive. If the row for the previous words does not exist in our table, then we remove the oldest of the previous words and try with the remaining $i-1$ words, we do this until a row is found. This is the exact reason why when creating our generalized n-gram we decided to store all i-grams where $i \in \lbrace 1,...,n \rbrace$. Our backoff is relatively naive as it does not backoff in the case of low probabilities, instead backing off only if absolutely necessary.

\subsection{Add-One Smoothing}
Our description of add-one smoothing is described in the smoothing section, it is implemented in our code as: \begin{verbatim}
addonefun = lambda i,nv: 1 + nv
\end{verbatim}

\subsection{Right-to-Left n-grams}
We have some sample sentences generated right-to-left in the next section.\par
In order to implement n-grams that used the right-to-left ordering of words instead of left-to-right we had to change a few things. First to clarify: right-to-left means if the trigram `a cow moos' was in the training corpus, `a' would be stored with a count in the row for `moos cow' instead of `moos' being stored in the row for `a cow'. The first thing to do was after pulling the training corpus into the list, we completely reversed the list. After that processing to generate the model could occur as before. We also had to change our perplexity and randomSentence functions slightly. Our perplexity function has an added conditional that reverses the test corpus if the model is right-to-left. Our randomSentence function runs the same way as before, but if the model is right-to-left the final output of the sentence has to be reversed.

\section{Experimentation}

\textbf{Bible.train and bible.test perplexity scores:}\\
Good Turing: 159.791135281 (bigram)\\
Good Turing: 158.595972171 (bigram, RL)\\
Add One: 280.215776754 (bigram)\\
Good Turing smoothing: 243.432299431 (unigram)\par

\subsection{Sample bigram sentences and 7-gram sentences from the Bible corpus}
Since we are able to generate n-gram sentences for any reasonable $n$, we decided to compare the output of bigram vs 7-gram sentences for the Bible corpus as a demonstration. For all these sentences, we've applied Good-Turing smoothing. (Our sample generated sentences are included in \texttt{sentenceGenerationSampleOutputs.txt}.
\begin{verbatim}
    ngram('bible.train',2,Smooth.GOOD_TURING,False)}
    ngram('bible.train',7,Smooth.GOOD_TURING,False)}
    ngram('raw_reviews.train',2,Smooth.GOOD_TURING,False)}
    ngram('raw_reviews.train',7,Smooth.GOOD_TURING,False)}
\end{verbatim}

\begin{adjustwidth}{0.25in}{0.5in}
\setlength{\parindent}{-0.15in}
Here are four sentences (first two bigram, next two 7-gram) from the Bible corpus:\par
{\small
\texttt{<s> thou hast thou shalt make the amalekites and to death this that are with their possession . and isaac , in wait sheminith stool , saying , he sent me and his wife , and blessed my voice , and with him unto him out of silver , and he lighted benjamite , and the lord descended swifter than the sanctuary : lo , <s>}\par
\texttt{<s> and the man of the altar most holy , in with his father , and joab : for an atonement for they shall be next day of reuben heard in their drink , that the priest , and out , even for us . <s>}\par\smallskip

\texttt{<s> then all the children of israel , and say unto them , when ye come into the land whither i bring you , <s>}\par
\texttt{<s> and god made the beast of the earth after his kind , and cattle after their kind , and every thing that creepeth upon the earth , wherein there is life , i have given every green herb for meat : and it was so . <s>}\par\smallskip
}
We can see that the latter two sentences have fewer grammatical issues, and fewer discontinuities of ideas. However, we can also note that in some parts of the 7-gram sentences are exact copies of expressions in the corpus: for example, ``God made the beast of the earth after his kind, and cattle
after their kind, and every thing that creepeth upon the Earth'' was an exact replica! This is not much of a surprise, even after smoothing, because our 7-gram model is so sparse.
\end{adjustwidth}

\subsection{Right-to-left trigram sentences vs.\ left-to-right trigram sentences}
\begin{verbatim}
    ngram('bible.train',3,Smooth.GOOD_TURING,False)}
    ngram('bible.train',3,Smooth.GOOD_TURING,False,Direction.RL)}
\end{verbatim}

\begin{adjustwidth}{0.25in}{0.5in}
Here are 4 sentences generated from the Hotel corpus, 2 generated traditionally left-to-right, and 2 generated backwards, right-to-left.\par
{\small
\texttt{<s> i called the reservation experience ; to find that the house keeper only did the stove and lcd hd tv were amongst the standard items . <s>}\par
\texttt{<s> for the trouble of filling it out before we were moved to another customer . <s>}\par\smallskip
\texttt{<s> the pool ; and as relaxing as possible . <s>}\par
\texttt{<s> the first 6 floors of the lake and the no taxation without libations promotion ; so our main reason i would give it a try . <s>}
}\par
It seems very difficult to see any strikingly different qualities between these sentences generated forwards/backwards\ldots if we had a bit more time, perhaps we may have some additional insights into how these random sentences are different. As of now, we are unable to say conclusively whether this backwards ngram model would have any significance on perplexity (see perplexity scores above), or whether predicting past words make better sentences.
\end{adjustwidth}

\section{Division of work}
We divided up our tasks as follows. While we did split up responsibilities, each of us contributed well to all other aspects of the programming and writeup.

\begin{itemize}[noitemsep]
\item Ben Shulman: Generalized n-gram, Backoff, Right-to-left n-grams and random sentence generation; Perplexity
\item Spandana Govindgari: Parsing Hotel Reviews; Hotel Reviews truthfulness prediction
\item Andy Wang: Good-Turing smoothing and add-one smoothing, Handling unknown words
\end{itemize}

\end{document}
