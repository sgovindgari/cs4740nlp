Notes on the project

1. Implement a collection of n-gram models
2. Specify responsibilities of each member

last 2 questions in writeup:
  - Can you perform any experiments that support the need for extension of change?
  - What experiments to show whether or not your extension has desired effect?

==Programming portion==
 1. Unsmoothed n-grams (COMPLETE BY BEN - might want to discuss parsing a little bit)
    Compute unsmoothed unigrams and bigrams
    Strip away labels and aggregate only text

    Bible - XML style - requires small amount of pre-processing
      Individual sentences in the Bible are separated by newlines
    Hotel review - process sentence boundaries with sentence segmentation tool.
      Include sentence boundaries as tokens
      Predict truthfulness of hotel reviews based on model
      IsTruthful and IsPositive - binary labels
      review - text of the review
      Predict review truthfulness and include a table that shows accuracy of the approach on the validation data
 2. Random sentence generation (COMPLETE BY BEN)
 3. Smoothing; unknown words (ANDY)
    Implement Good-Turing smoothing (COMPLETE by ANDY?)
      NOTES:
        self.uniques = list(set(self.corpus))
        http://www.youtube.com/watch?v=XdjCCkFUBKU
    Handle unknown words
 4. Perplexity (BEN)
    Compute and report it
 5. Open ended extension (COMPLETE)
    Choose one (or more) of:
    - trigram/4-gram/n-gram (complete? backoff?)
    - smoothing (ANDY: Add-one)
    - interpolation
    - nontrivial unknown word handling
    - employ the language model in the service of another NLP or speech application
    - implement a modification that makes use of the validation set
 5.7 (not optional) - Hotel Review prediction (SPANDANA)

==Report (6 pages)==
  Contain every step of the programming portion
	- Describe your approach
	- Include relevant examples where appropriate
	- Include examples of random generator in action
	- indicate which smoothing you implemented
	- How you handled unknown words?
	- Discuss results of the perplexity experiments
	- Describe the extensions that you implemented and why
	- Can you perform any experiments that support the need for extension of change?
	- What experiments to show whether or not your extension has desired effect?

  Code
	- Include snippets with description of approach
	- Relevant portions of the code
	- Dont include irrelevant code
  Short section that explains how you divided up the work

Grading Guide
  10% - Part 1 submission: progress on unigram and bigram table algorithms
                           and random sentence generation
  10% - design and implementation
  05% - Random sentence generation
  25% - Extension
  50% - Report (experiment/testing design+methodology, clarity, quality)

================================================================================
How ngram works:
  * Takes the passed filename and reads it into a list
  * If we are using a RL ngram instead of LR it reverses the list
  * If we are using unk tokens we replace the first occurrence of each unique word in our corpus with <unk> -- taken from book
  * We then create a table for all necessary counts for each i-gram, i = 1,...,n
    * It does not create an entry for words that have no count (i.e. if cow never follows brown, but cow is in the vocab, it won't have an entry in the bigram row for brown)
  * We then generate the appropriate smoothing algorithm for generating probabilities
  * Probabilities for seen i-grams i=1,...,n are then calculated using the appropriate smoothing method
  import nltk # http://www.nltk.org/install.html
              # in python interpreter: nltk.download() to get punkt
