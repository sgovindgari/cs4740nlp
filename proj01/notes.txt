Notes on the project

1. Implement a collection of n-gram models
2. Specify responsibilities of each member

==Programming portion==
 1. Unsmoothed n-grams (COMPLETE BY BEN - might want to discuss parsing a little bit)
    Compute unsmoothed unigrams and bigrams
    Strip away labels and aggregate only text

    Bible - XML style - requires small amount of pre-processing
      Individual sentences in the Bible are separated by newlines

    Hotel review - process sentence boundaries with sentence segmentation tool.
      Include sentence boundaries as tokens
      Predict truthfulness of hotel reviews based on model

      IsTruthful and IsPositive - binary labels
      review - text of the review

      Predict review truthfulness and include a table that shows accuracy of the approach on the validation data

    ===Corpuses===
      King James Bible
      Hotel reviews from Amazon

 2. Random sentence generation (COMPLETE BY BEN)
    Code for generating random sentences based on unigram or bigram model
    Include examples of sentence generated by your sentence in part 1 submission
======================================================================================== (Part 1)
 3. Smoothing; unknown words (ANDY)
    Implement Good-Turing smoothing (COMPLETE by ANDY?)
      NOTES:
        self.uniques = list(set(self.corpus))
        tuples = list(itertools.product(*[self.uniques for _ in range(self.n - 1)]))
        # impossible. too much mem usage, paging to disk too much!
        N_0 is #wordsonce / total
        http://www.youtube.com/watch?v=XdjCCkFUBKU
    Handle unknown words

 4. Perplexity (BEN)
    Implement code that computes perplexity
    Compute and report it

 5. Open ended extension (COMPLETE)
    Choose one (or more) of:
    - trigram/4-gram/n-gram (complete? Katz's backoff?)
    - smoothing (ANDY: linear interp)
    - interpolation
    - nontrivial unknown word handling
    - employ the language model in the service of another NLP or speech application
    - implement a modification that makes use of the validation set

 5.7 (not optional) - Hotel Review prediction (SPANDANA)

==Report (6 pages)==
  Contain every step of the programming portion
	- Describe your approach
	- Include relevant examples where appropriate
	- Include examples of random generator in action
	- indicate which smoothing you implemented
	- How you handled unknown words?
	- Discuss results of the perplexity experiments
	- Describe the extensions that you implemented and why
	- Can you perform any experiments that support the need for extension of change?
	- What experiments to show whether or not your extension has desired effect?

  Code
	- Include snippets with description of approach
	- Relevant portions of the code
	- Dont include irrelevant code
  Short section that explains how you divided up the work

Grading Guide
  10% - Part 1 submission: progress on unigram and bigram table algorithms
                           and random sentence generation
  10% - design and implementation
  05% - Random sentence generation
  25% - Extension
  50% - Report

================================================================================
How ngram works:
  * Takes the passed filename and reads it into a list
  * If we are using a RL ngram instead of LR it reverses the list
  * If we are using unk tokens we replace the first occurrence of each unique word in our corpus with <unk> -- taken from book
  * We then create a table for all necessary counts for each i-gram, i = 1,...,n
    * It does not create an entry for words that have no count (i.e. if cow never follows brown, but cow is in the vocab, it won't have an entry in the bigram row for brown)
  * We then generate the appropriate smoothing algorithm for generating probabilities
  * Probabilities for seen i-grams i=1,...,n are then calculated using the appropriate smoothing method
=======
The first 46 words in the bible corpus, represented in self.counts
  (24 distinct words = 24 unigrams)
  unigram: add 1 to all (unknowns?)
  bigram: add 1 to all (all unigrams. unknowns?)
  ..

====
  extensions: #7 Programming Project is not optional!!
  import nltk # http://www.nltk.org/install.html
              # in python interpreter: nltk.download() to get punkt
  extension: make an n-gram model going right-to-left, and note differences in output?

  ben's ngram implementation:
                | "cow" | "horse" | ..
    (the,brown) |  30   |   20    | ..
    (a, dark)   |  20   |   100   |
    for probabilities: generate 0..1. traverse sorted dict, subtract amt, if <0.
