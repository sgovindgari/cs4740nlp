Notes on the project

1. Implement a collection of n-gram models
2. Specify responsibilities of each member

==Programming portion==
 1. Unsmoothed n-grams (COMPLETE BY BEN - might want to discuss parsing a little bit)
    Compute unsmoothed unigrams and bigrams
    Strip away labels and aggregate only text

    Bible - XML style - requires small amount of pre-processing
      Individual sentences in the Bible are separated by newlines

    Hotel review - process sentence boundaries with sentence segmentation tool.
      Include sentence boundaries as tokens
      Predict truthfulness of hotel reviews based on model

      IsTruthful and IsPositive - binary labels
      review - text of the review

      Predict review truthfulness and include a table that shows accuracy of the approach on the validation data

    ===Corpuses===
      King James Bible
      Hotel reviews from Amazon

 2. Random sentence generation (COMPLETE BY BEN)
    Code for generating random sentences based on unigram or bigram model
    Include examples of sentence generated by your sentence in part 1 submission
======================================================================================== (Part 1)
 3. Smoothing; unknown words (ANDY)
    Implement Good-Turing smoothing
    Handle unknown words

 4. Perplexity (BEN)
    Implement code that computes perplexity
    Compute and report it

 5. Open ended extension (COMPLETE)
    Choose one (or more) of:
    - trigram/4-gram/n-gram (complete?)
    - smoothing (ANDY: linear interp)
    - interpolation
    - nontrivial unknown word handling
    - employ the language model in the service of another NLP or speech application
    - implement a modification that makes use of the validation set

 5.7 (not optional) - Hotel Review prediction (SPANDANA)

==Report (6 pages)==
  Contain every step of the programming portion
	- Describe your approach
	- Include relevant examples where appropriate
	- Include examples of random generator in action
	- indicate which smoothing you implemented
	- How you handled unknown words?
	- Discuss results of the perplexity experiments
	- Describe the extensions that you implemented and why
	- Can you perform any experiments that support the need for extension of change?
	- What experiments to show whether or not your extension has desired effect?

  Code
	- Include snippets with description of approach
	- Relevant portions of the code
	- Dont include irrelevant code
  Short section that explains how you divided up the work

Grading Guide
  10% - Part 1 submission: progress on unigram and bigram table algorithms
                           and random sentence generation
  10% - design and implementation
  05% - Random sentence generation
  25% - Extension
  50% - Report
