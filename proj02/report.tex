%!TEX TS-program = pdflatex or xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass{article}
\usepackage[letterpaper, margin=1in, left=0.75in, right=0.75in] {geometry}
\usepackage{microtype} % micro appearance
\usepackage[all]{nowidow} % no lone lines
\usepackage{changepage} % changes layout mid-page
\usepackage{enumitem} % enum (itemize, enumerate, description)
\usepackage{ifthen}
\usepackage{xfrac}
\usepackage{fancyhdr} % headers
\usepackage{amsmath} % matrices
\usepackage{amssymb} % symbols
\usepackage{relsize}
\usepackage{gensymb} % symbols
\usepackage{tikz} % graphics?
\usepackage{bm} % bold math
\usepackage{multicol} % multiple columns
\usepackage{pgfplots}
\usepackage{lipsum} % lipsum
\usepackage{hyperref}

\pagestyle{fancy}

\newcommand {\classname} {CS 4740/5740 -- Intro to Natural Language Processing}
\newcommand {\duedate} {Wed, 2014--03--19}
\newcommand {\hwtype} {Project}
\newcommand {\hwnum} {2: Word Sense Dab.}
\newcommand {\innermargin} {0.15in} % indentation amt of nested lst

\let\tss\textsuperscript
\let\oldunderline\underline
\renewcommand\underline[1]{\oldunderline{\smash{#1}}}

\fancyhead[L] {\ifthenelse {\thepage=1}
  {bgs53 (Ben Shulman), sg754 (Spandana Govindgari),\\
   ms969 (MJ Sun), amw275 (Andy Wang)}
  {\hwtype\ \hwnum\ (bgs53, sg754, ms969, amw275)}}
\fancyhead[R] {\ifthenelse {\thepage=1}
  {\classname\\\hwtype\ \hwnum\ (due \duedate)}
  {Page \thepage}}

\begin{document}
\begin{center}\textbf{Project 2: Report on Word Sense Disambiguation}\end{center}

\section{Introduction}

For this project we have written our code in Python. We used several of python's included libraries as well as tools from NLTK. We used NLTK's WordNetLemmatizer for lemmatizing words in our data, NLTK's maximum entropy POS tagger based off the Penn TreeBank was used for part-of-speech tagging of data, finally we used NLTK's corpus of stopwords to eleminate stopwords from the data sets. To run our code, please reference our README.

\section{Preprocessing}

\subsection{Dataset Cleanup}

To clean up the training, validation and test data we began by writing a function \texttt{cleanFile} which took a source file and wrote the cleaned version to a specified destination. For each line of the source file the function cleans the words before and words after the target word in the file. For those before and after words we have a \texttt{clean} function which for each word checks if the word is a punctuation, or a stopword, if so the word is thrown out. Otherwise the word is lemmatized and retained. This is done using a single line of code as follows:

{\small
\begin{verbatim}
return [lemma.lemmatize(word) for word in words if
  (re.match('.*\w+.*',word) != None) and (word not in stopword.words('english'))]
\end{verbatim}
}

Data cleanup was also needed at various moments of computation for the dictionary-based system. The function \texttt{cleanString} takes a string as input, and outputs a lemmatized, de-puncuated string without stopwords.

\subsection{Dictionary Cleanup}
For the dictionary-based system, our input file \texttt{dictionary.xml} needed to be slightly reformatted for the Python library \texttt{lxml} to accept and parse correctly. In \texttt{utilities.py}, the function \texttt{fixDoubles} preprocesses the dictionary, fixing the instances of double senses. For example, one of ``drug'''s senses references its previous senses 1 and 2: ``1\&\&2''. We simply dropped this dictionary entry as it would not affect the overall performance of our system without it. We also removed instances of double-quote characters in our examples, as that was messing up the XML parser, and those characters were not needed.

\texttt{fixDoubles} generates \texttt{dictionary\_processed.xml}, which we read in using Python's lxml library.

\section{Naive Bayes}

\subsection{Feature Vector Construction}

To construct features we wrote a function: \texttt{constructSet}. This function took a source as well as parameters specifying window size; whether to use co-locational, co-occurrence features or both; whether to use the part of speech of the target word in the example as a feature; and what count function to use for co-occurrence features. Co-occurrence features could be counted using either a boolean count (does the word appear or does it not), or a simple count (how many times the word appears). For each line of the source file we would create an entry in a list. Each entry is a three tuple made up of the target word followed by its sense, followed by a dictionary of features. The dictionary's keys are the feature and the value is the value of that feature in the example. 

We extracted the target word, sense and examples by using a simple regex to get a list of the parts. Part-of-speech of the target word was extracted from the first part of the example which is structured as: \texttt{<word>.<pos>}. For each word within the window we created a co-occurrence feature in the example's feature vector where the key is the word and the value is the count of it (or simple boolean, did it appear or did it not). For each word in the window we also extracted co-locational features. For instance if the 5th word following the target word was ``jaguar'' a feature would be added to this example's feature vector. The key would be "(after,5) and the value would be the word. We also pulled the POS of that word creating a feature with key (after-pos,5) and value being the pos of the word, as tagged by our pos-tagger from NLTK.

\subsection{Model Construction}

In order to construct a naive bayes model (class \texttt{naive\_bayes} in our code) of the examples generated in the previous section, we generated the appropriate counts necessary. In our naive bayes' constructor we iterate over all passed in examples. We maintain a list of word counts in \texttt{self.wordcounts} which is a dictionary mapping target words to how many examples are for that word. We also maintain \texttt{self.featureLists} which is a dictionary which maps target words to sets of all features for each target word, each set is created as we iterate through examples and gather all features from each example for each word. Further we maintain a dictionary of dictionaries holding sense counts. For each word we have an entry in \texttt{self.senseCounts} which holds a dictionary where the keys are the sense and the value are the number of times that sense appears in the training set. 

Finally we create \texttt{self.featureCounts}, this is a dictionary with keys of (word,sense). Then for each key there is a dictionary with keys corresponding to each feature. These features values are dictionaries which have a key for each value for that feature that appears in the training set with the number stored in the dictionary being the count of the number of times the feature with the specific value has shown up for that (word,sense) pair in the training set. To illustrate this, consider the case when we have two following feature vectors for the word ``affect'' with the sense ``1'':
{\small
\begin{verbatim}
{"f1" : "dog", "f2" : 3, "f3" : "hello"}
{"f1" : "dog", "f2" : 2, "f4" : "boat"}
\end{verbatim}
}
In this case \texttt{self.featureCounts} would have one entry: ("affect","1"), this entry would look as follows:
{\small
\begin{verbatim}
{
 "f1" : { "dog" : 2 }
 "f2" : { 2 : 1, 3 : 1 }
 "f3" : { "hello" : 1 }
 "f4" : { "boat" : 1 }
}
\end{verbatim}
}

It is important to note that we do not maintain count of features which are part of a specific word but do not appear in an example. For instance the second example in our previous case is missing the feature ``f3'' so in actuality it has a feature of ``f3'' with a value of null (0 if it was a boolean/number feature), but we do not count this as we can generate that count on the fly using the counts we maintain in \texttt{self.featureCounts} and \texttt{self.senseCounts}.

\subsection{Classification}

In order to classify a set of examples we wrote a function \texttt{classify} which takes a testSet (one processed by the function \texttt{constructSet} described previously). It also takes an alpha, the value used in smoothing; softscore, whether to use softscoring or not; and kaggle, whether to write a file compatible with kaggle storing results. In \texttt{classify} we go over each example and generate a probability for each sense the target word has in our training set. We generate the probabilities by going over each feature in our \texttt{self.featureLists} for the target word.

We pull the value for that feature from our test example if it has one, if it doesn't then the value for that feature is null/0. We then find the corresponding entry in \texttt{self.featureCounts} if the feature is not null/0 in this example, if the entry does not exist (i.e. the value does not occur in the training set for this sense) then we add\par
\texttt{math.log(alpha/float(sc+alpha*len(self.featureLists[word])))}\\
to our log probability--sc is the sense count. If the entry does exist, replace alpha with the count in \texttt{self.featureCounts} plus alpha in the above formula. If the feature in the test example has null/0 value then we find how many examples in the training set correspond to that value by counting how many do not have that value and subtracting that number from the sense count, the code for that addition to log probability is as follows:\par
\begin{verbatim}
    math.log((sc-sum(self.featureCounts[(word,sense)][key].values())+alpha) /
        float(sc+alpha*len(self.featureLists[word])))}.
\end{verbatim}
If there are no training examples with non-null/0 value then the numerator in the above code becomes \texttt{(sc+alpha)}.

The log probability for each training example also has the log of \texttt{(sc/wc)} added to it, where sc is sense count and wc is word count for the target word. Thus for each example we calculate an approximation (the log of an approximation) of:
$P(s_i)\mathlarger{\prod}_{j=1}^{n}P(f_j|s_i)$ for our model. We approximate these probabilities using $P(s_i) \approx count(s_i|w)/count(w)$ and $P(f_j|s_i) \approx count(f_j|s_i,w)/count(s_i|w)$. We use the log of probability to avoid underflow. Further we use add-alpha smoothing to avoid having counts of zero which are negative infinity in logs (or 0 for probability, meaning the sense would never be selected). This is especially important because the feature vectors are often very sparse.

We do this for each sense of each target word example and take the argmax to produce a prediction. We do this for each example in the test set and then output a tuple of our accuracy and a list of tuples corresponding to \texttt{(actual\_class,predicted\_class)} for each example. 

\subsection{Soft-scoring}
When calculating soft-scoring for accuracy, we process the classification probability scores of each word after they've been calculated using the Naive-Bayes model. In order to prevent underflow, we scale all the probability scores $pr$ of each word first by modifying them to become $e^{pr-maxProb}$ where $maxProb$ is the maximum probability for that word. We then normalize the scores to calculate the actual probability of each sense prediction. Then, during calculation of accuracy, instead of adding 1 for each correct prediction, we add the predicted probability for the golden standard sense.


\section{Dictionary-based}

\subsection{Dictionary Construction}
In our Dictionary based WSD, we made use of both the given dictionary as well as WordNet in order to derive meanings of words. (We should note that we are using the default WordNet version 3.0, not 2.1, in our derivations. This turns out to be okay, since we are not using the provided wordnet sense integers.)

In \texttt{utilities.py}, \texttt{fixDoubles} generates \texttt{dictionary\_processed.xml}, a preprocessed dictionary for which we can read in using Python's lxml library. From here, we transform the data into a dictionary, a lookup-table where the word is the key. The value, if the word is found, is the tuple of (part of speech, sub-dictionary). The sub-dictionary was further a map with key as the senseID and value as another tuple of (definition, example list, and wordNet Senses). Whenever a word was not found in the dictionary, we used WordNet to extract the word's senses and definitions, and then updated our dictionary to contain this word.

\subsection{Implementation of Lesk Algorithm}
To disambiguate words with dictionary approach, we used the Lesk algorithm that computed the number of overlaps between the definitions of the target word and context words. In order to find the best sense, we first get the definition of our target word by searching it in our dictionary. If the word is not found, we search WordNet and update dictionary accordingly. Once we know the list of senses, each sense is used in computeOverlap function to calculate the \emph{distant overlaps, consecutive overlaps, and context overlaps}. We made a few optimizations to this function in order to pick relevant words from the context and calculate overlap which you can find in the section below. Each overlap score is compared to the maximum overlap and the best sense is returned which is attached to the maximum overlap.

\subsection{Optimizations to Lesk Algorithm}
The optimizations we made to Lesk Algorithm mostly revolve around the \texttt{computeOverlap} function. Our function takes in the target word, sense definitions and examples and context words within a certain window to calculate overlaps. In order to find relevant words, we kept our window to a default 10 words and further experimented by increasing and decreasing the window. We used three overlap measures to calculate the overlap rewarding each one appropriately.

The overlap score computes the number of individual words that overlap between the definitions of two words. The consecutive overlap score computes the number of consecutive overlaps that occur in between the definitions. And finally, context overlap as we call it is a measure of the number of words from the context that overlap with the definitions. We used the context overlap measure because we noticed that words used within context sometimes do overlap with the definition of the target word. With these three measures, our metric system to calculate the overlap score is as follows:
\begin{align}
overlap = 15*consecutive\_overlaps + 5*overlap + 1*context\_overlap
\end{align}

\subsection{Soft-scoring}
To do soft-scoring for our dictionary implementation we had to find a way to generate confidence in each sense. To do this we simply normalized our overlaps scores we calculated for each sense. To account for senses that might have zero overlap we used a small alpha smoothing parameter so they did not have zero probability under our soft-scoring model. We then compute accuracy in the same way as we did for the Naive Bayes model.

\section{Experimentation}

\subsection{Na\"{i}ve-Bayes}

We tested our implementation of Naive Bayes in various ways using the validation data. The first testing we did was determine our baseline: simply predicting the most common sense for each word; this gave an accuracy of 81.14\%. Then we tested co-occurrence and co-locational features and the part-of-speech feature. Because the training data for each word is very biased and very minimal (for instance `capital' occurs 278 total times: 258 for sense 1, 18 for sense 2, 1 for sense 3, 1 for sense 5) we stopped multiplying the probability for each sense by the probability of the sense occurring in the training set for that word. Thus we are assuming that the training data is not biased at all for our testing of features. We quickly realized that the POS feature had the same value for all training examples for each word, making our Naive Bayes simply choose the first feature every time, resulting in an accuracy of only 55.84\%. We tested co-occurrence features for window sizes from 1 to about 20, but stopped going beyond that because for all window sizes we got the same accuracy: 81.35\%, a slight improvement above the baseline, but very minimal. Our testing of co-location features were more fruitful, showing poor performance at the beginning, but improving quick as window size increased, eventually peaking at 82.21\% (see \hyperref[fig:col]{Figure 1}).

\begin{figure}[h]\label{fig:col}
\centering
\begin{tikzpicture}
  \begin{axis}[]
    \addplot table [x index=0, y index=1, col sep=comma] {col.csv};
  \end{axis}
\end{tikzpicture}
\caption{Size of window vs. accuracy for co-locational features}
\end{figure}

We also tested how the accuracy of our Naive Bayes model would change on the validation set as we increase the maximum possible number of training examples for each sense of each word. For this test we used a Naive Bayes model using only co-occurrence features with a window size of 10. We tested both soft-scoring and hard-scoring. We found that accuracy started fairly low for both and as training size increased, accuracy increased quickly for both scoring systems, with soft-scoring always slightly lower than hard (see \hyperref[fig:size]{Figure 2}).

\begin{figure}[h]\label{fig:size}
\centering
\begin{tikzpicture}
  \begin{axis}[
  legend pos = outer north east,]
    \addplot table [x index=0, y index=1, col sep=comma] {size.csv};
    \addlegendentry{hard-scoring};
    \addplot table [x index=0, y index=2, col sep=comma] {size.csv};
    \addlegendentry{soft-scoring};
  \end{axis}
\end{tikzpicture}
\caption{Maximum size of training set for a single sense vs. accuracy for a naive bayes model with window size 10 using only co-occurrence features.}
\end{figure}

\subsection{Dictionary-based}

TODO

\section{Discussion}

The data provided to us, in particular the training set and validation set, made it a struggle for the supervised system to perform well. The task at hand essentially required a separate Naive Bayes model for each target word, meaning that data was sparse. The training set had a median of only 128.5 examples per word, and a mean of 222.81. The max wasn't that large, at 2,536 examples for one word, and the minimum was incredibly low at 19. This means that there were very few examples for each word. To make matters more difficult most of the data was very biased. For instance, `capital' had 278 total examples: 258 for sense 1, 18 for sense 2, 1 for sense 3, and 1 for sense 5. Because of this bias in the data it made it very hard for Naive Bayes to create an accurate model of what words might be important in context for most senses of most target words. We believe this is the reason we struggled to score much above the baseline using our supervised model. 

After analyzing our models we found that the most important class of features were the part-of-speech co-locational features. Using only these features we achieved an accuracy of 82.32\% on the validation set (window size 90). Second best were word co-locational features which achieved an accuracy of 81.89\% on the validation set (window size 90). Third best were co-occurrence features which had an accuracy of 81.35\% (window size 90) on the validation set. We believe that POS co-locational features were best because they generalized more than word co-locational features, as having a noun at the 5th position following the target word is more general than having `cat' at that same position as a feature. We believe that co-locational features were probably powerful than co-occurrence features because they are for specific positions rather than general words over the whole window.

%TODO: Discuss Dictionary 

The supervised approach uses training data to create a model of the probability of features drawn from those training examples (such as co-occurrence and co-locational features) to determine how well a test example fits the model for a certain sense. The dictionary approach uses already existing dictionaries to compare how well a test example's in-context words' definitions overlap with the definitions for the target word's senses to pick which sense is most likely. The supervised approach has the advantage of doing very well with large amounts of training data in comparison to a dictionary based approach which will typically not do as well, given that there is adequate training data for the Naive Bayes model. However, this makes Naive Bayes very susceptible to problems with training data. For instance, having a training set which is very biased towards one sense or another, or is not representative of real uses of a specific sense can make the model very inaccurate. Further often there is not enough data to provide a supervised learning approach an adequate representation of the general use of a sense. This means the model will be very specific for each sense and thus not generalizable. The dictionary approach solves this by using already existing dictionaries to determine likelihood thus not relying on getting adequate data. 

%Some statistics: 
%Training set:
%Previous::: mean: 31.0285444998 median: 31.0 max: 120
%After::: mean: 29.7005520399 median: 29.0 max: 114

%Validation set:
%Previous::: mean: 31.436227224 median: 31.0 max: 83
%After::: mean: 31.1232583065 median: 31.0 max: 79

%Test Set:
%Previous::: mean: 31.7912200102 median: 31.0 max: 84
%After::: mean: 30.6061766207 median: 30.0 max: 104

%'exchange':
%{1: 49, 2: 3, 3: 261, 4: 1, 5: 49}

%accuracy for col with windowsize 90: 0.822079314041 

\end{document}
