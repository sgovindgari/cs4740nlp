%!TEX TS-program = pdflatex or xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass{article}
\usepackage[letterpaper, margin=1in, left=0.75in, right=0.75in] {geometry}
\usepackage{microtype} % micro appearance
\usepackage[all]{nowidow} % no lone lines
\usepackage{changepage} % changes layout mid-page
\usepackage{enumitem} % enum (itemize, enumerate, description)
\usepackage{ifthen}
\usepackage{xfrac}
\usepackage{fancyhdr} % headers
\usepackage{amsmath} % matrices
\usepackage{amssymb} % symbols
\usepackage{relsize}
\usepackage{gensymb} % symbols
\usepackage{tikz} % graphics?
\usepackage{bm} % bold math
\usepackage{multicol} % multiple columns
\usepackage{lipsum} % lipsum

\pagestyle{fancy}

\newcommand {\classname} {CS 4740/5740 -- Intro to Natural Language Processing}
\newcommand {\duedate} {Thu, 2014--03--06}
\newcommand {\hwtype} {Project}
\newcommand {\hwnum} {2: Word Sense Dab.\ (proposal)}
\newcommand {\innermargin} {0.15in} % indentation amt of nested lst

\let\tss\textsuperscript
\let\oldunderline\underline
\renewcommand\underline[1]{\oldunderline{\smash{#1}}}

\fancyhead[L] {\ifthenelse {\thepage=1}
  {bgs53 (Ben Shulman), sg754 (Spandana Govindgari),\\
   ms969 (MJ Sun), amw275 (Andy Wang)}
  {\hwtype\ \hwnum\ (bgs53, sg754, ms969, amw275)}}
\fancyhead[R] {\ifthenelse {\thepage=1}
  {\classname\\\hwtype\ \hwnum\ (due \duedate)}
  {Page \thepage}}

\begin{document}
\begin{center}\textbf{Project 2: Word Sense Disambiguation -- Proposal}\end{center}

Our code for the entire project will be implemented in Python using nltk and lxml libraries as well as models we built in Project 1

\section{Our two WSD systems}

\begin{itemize}
\item \textit{What kinds of features are you planning to extract from the surrounding context for supervised WSD?}\par

For pre-processing we plan on stripping out stem words like the, are, our, we etc and stem the context words so that words like office and offices are treated the same (offic). The features we are going to extract after this pre-processing is done are: part-of-speech to narrow down, tense, plurality and example sentences. The number of previous and next context words we will be using is passed in as a parameter to go left or right. If negative, take all, otherwise. The context words are going to be a function of weighted distance (1/d) so that the word that is farther away from our target word has lesser weight than the word that is closer to it. After training, our feature vector will have the following form:  
Lookup Table (Target word, Array of Vectors)\par
Each Vector will be of this form: \par
label - $\langle$POS, next-word, next-word, next-word, etc (counts), prev-word, prev-word, etc (counts)$\rangle$

We plan on using scipy which might help with performance since we will be building sparse matrices

\item \textit{What are you going to do for finding relevant words in the context for
dictionary-based WSD?}\par

Again we plan on stripping out stop words and use stemming to preprocess our dictionary examples and training data as well. Given the word, we store all the definitions and examples in a table with the key as the sense-id. This will be the signature to our \texttt{computeOverlap} function. The function will also take context that will contain 5-10 words as the window (which will be inputted as a parameter). We will have a table that contains (\(context word, sense-id\) , \# of words that overlap, \# of consecutive words that overlap). Overlapping basically means the occurrence of exact/similar words after stemming in both the definitions of context words and target word. After we build this table, we will compute a score by weighing the consecutive overlaps more that distant overlaps. This parameter will be tuned based on validation and training set. Once the score is computed, we identify the sense-id by the highest ranking tuple.

\item \textit{Provide a clear and brief explanation of your planned systems and algorithms. (Do not repeat the basic models that are already described in this document) Rather than writing up every single detail, try to explain the motivation of your design decisions by investigating the provided dataset and illustrating the intuition that you discovered from the real examples. Note that better features (typically identified by looking at the training data), the higher accuracies you will likely achieve.}\par

Preprocess using stemming. Strip out stop words. Feature extraction. Naive: pos, $n$ words around the word, and counts. Weighted regression/averaging.

\end{itemize}

\section{Implementation schedule}
\textit{Provide a brief implementation schedule.}\par

Division of labor:
\begin{itemize}[noitemsep]
\item MJ: parse data, naive-bayes
\item Ben: naive-bayes
\item Andy: dictionary-based wsd
\item Spandana: dictionary-based wsd
\end{itemize}

Extensions:\par
Do three of four! (Spandana read it on Piazza)

\begin{itemize}[noitemsep]
\item Sat, March 8: most necessary preprocessing is done.
\item Sun, March 9: corrections
\item Tue, March 11: initial data structures for dictionary WSD and supervised wsd underway, some initial testing
\item Fri, March 14: dictionary WSD implementation done, more testing
\item Sun, March 16: supervised WSD implementation done, begin report writeup
\item Mon, March 17: validation and testing for both systems, upload to Kaggle
\item Tue, March 18: any corrections...\ and report to be completed.
\end{itemize}

\end{document}
