%!TEX TS-program = pdflatex or xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass{article}
\usepackage[letterpaper, margin=1in, left=0.75in, right=0.75in] {geometry}
\usepackage{microtype} % micro appearance
\usepackage[all]{nowidow} % no lone lines
\usepackage{changepage} % changes layout mid-page
\usepackage{enumitem} % enum (itemize, enumerate, description)
\usepackage{ifthen}
\usepackage{xfrac}
\usepackage{fancyhdr} % headers
\usepackage{amsmath} % matrices
\usepackage{amssymb} % symbols
\usepackage{relsize}
\usepackage{gensymb} % symbols
\usepackage{tikz} % graphics?
\usepackage{bm} % bold math
\usepackage{multicol} % multiple columns
\usepackage{lipsum} % lipsum

\pagestyle{fancy}

\newcommand {\classname} {CS 4740/5740 -- Intro to Natural Language Processing}
\newcommand {\duedate} {Thu, 2014--03--06}
\newcommand {\hwtype} {Project}
\newcommand {\hwnum} {2: Word Sense Dab.\ (proposal)}
\newcommand {\innermargin} {0.15in} % indentation amt of nested lst

\let\tss\textsuperscript
\let\oldunderline\underline
\renewcommand\underline[1]{\oldunderline{\smash{#1}}}

\fancyhead[L] {\ifthenelse {\thepage=1}
  {bgs53 (Ben Shulman), sg754 (Spandana Govindgari),\\
   ms969 (MJ Sun), amw275 (Andy Wang)}
  {\hwtype\ \hwnum\ (bgs53, sg754, ms969, amw275)}}
\fancyhead[R] {\ifthenelse {\thepage=1}
  {\classname\\\hwtype\ \hwnum\ (due \duedate)}
  {Page \thepage}}

\begin{document}
\begin{center}\textbf{Project 2: Word Sense Disambiguation -- Proposal}\end{center}

Our code will be written in Python, will use nltk and lxml libraries (among others), and models from Project 1.

\subsection*{Our two WSD systems}

\begin{itemize}
\item \textit{What kinds of features are you planning to extract from the surrounding context for supervised WSD?}\par

For pre-processing we plan on stripping out stop words like the, are, our, we etc and stem the context words so that words like office and offices are treated the same (offic). The features we are going to extract after this pre-processing is done are: part-of-speech to narrow down, tense, plurality and example sentences. The number of previous and next context words we will be using is passed in as a parameter to go left or right. If negative, take all, otherwise, the context words are going to be a function of weighted distance (perhaps something like $1/d$) so that the word that is farther away from our target word has lesser weight than the word that is closer to it. After training, our feature vector will have the following form:
\[\text{Lookup Table (Target word, Array of Vectors)}\]
Each Vector will be of this form:
\[\text{label - $\langle$POS, next-word, next-word, next-word, etc (counts), prev-word, prev-word, etc (counts)$\rangle$}\]
We plan on using scipy which might help with performance since we will be building sparse matrices.

\item \textit{What are you going to do for finding relevant words in the context for
dictionary-based WSD?}\par

Again we plan on stripping out stop words and use stemming to pre-process our dictionary examples and training data as well. Given the word, we store all the definitions and examples in a table with the key as the sense-id. This will be the signature to our \texttt{computeOverlap} function (Lesk algorithm). The function will also take context that will contain 5-10 words as the window (which will be inputted as a parameter). We will have a table that contains (\(contextword, senseId\) , \# of words that overlap, \# of consecutive words that overlap). Overlapping basically means the occurrence of exact/similar words after stemming in both the definitions of context words and target word. After we build this table, we will compute a score by weighing the consecutive overlaps more that distant overlaps. This parameter will be tuned based on validation and training set. Once the score is computed, we identify the sense-id by the highest ranking tuple.

\item \textit{A brief explanation of systems and algorithms. Explain the motivation of your design decisions.}\par
We have described most of our plans for the supervised and dictionary-based WSDs above. Our general approach is:
\begin{itemize}[noitemsep,nolistsep]
\item Strip out stop words except when they are vital to a definition. (like ``the'', ``and'', etc, words that generally should have no effect on scores. This may result in higher accuracy in predictions.)
\item Pre-process examples using stemming. (Morphological roots are sufficient.)
\item Feature extraction (POS, $n$ words around the word, counts)
\item Weighted regression/averaging. (as also described in the supervised approach)
\end{itemize}

\end{itemize}

\subsection*{Implementation schedule}

\begin{itemize}[noitemsep]
\item Sat, March 8: most necessary pre-processing is done.
\item Tue, March 11: initial data structures for dictionary WSD and supervised WSD underway, some initial testing
\item Fri, March 14: dictionary WSD implementation done, more testing
\item Sat, March 15: supervised WSD implementation done, begin report writeup
\item Sun, March 16: a few extensions done
\item Mon, March 17: validation and testing for both systems, upload to Kaggle, final touches to extensions
\item Tue, March 18: any other corrections...\ and report to be completed.
\end{itemize}

Rough division of labor:
\begin{itemize}[noitemsep]
\item MJ \& Ben: Data parsing, Naive-Bayes
\item Andy \& Spandana: Dictionary-based WSD
\end{itemize}

\end{document}
