Links
http://www.nltk.org/_modules/nltk/tag/hmm.html
http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/hmms.pdf
http://mallet.cs.umass.edu/mallet-tutorial.pdf
http://mallet.cs.umass.edu/fst.php
http://svmlight.joachims.org/svm_struct.html
http://www.nltk.org/book/ch05.html
http://www.cs.cornell.edu/people/pabo/movie-review-data/
http://streamhacker.com/2010/06/16/text-classification-sentiment-analysis-eliminate-low-information-features/
http://streamhacker.com/2012/11/22/text-classification-sentiment-analysis-nltk-scikitlearn/
http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/
http://text-processing.com/docs/sentiment.html
http://text-processing.com/demo/sentiment/ -- Already implemented project

Goal - implement HMM 
or experiment with existing package/toolkit sequence tagging algorithm

Amazon review dataset
Training - 196
Test - 98

Review format

title
sentiment <space> text

Implementation
Each review = sequence of sentences
Each sentence tagged with sentiment

Using an exisiting toolkit
  - Run experiments
  - Think up in front what kind of experiments to run
  - Reserve some training for validation
  - Describe details of experimental designs in the report
  - Develop baseline systems
    (random guess,
     identify certain words frequently used for each sentiment,
     perform prediction on whether test includes these words)

Extensions
  (One extension is mandatory. More than one: counted as bonus!)
  - Experiment with different n-gram feature sets
  - Experiment with different smoothing methods
  - Compare one sequence tagging method with another
  - Vary feature settings to see how they affect performance of different methods
  - Implement secondary sequence tagging system 
  - Improve sentence level prediction task by incorporating document level sentiments

Proposal
  - Describe sequence tagging system
  - Implementation plan
  - Which model are you planning to implement?
  - Explain algorithmic key points
    (hidden variables, observed, corresponding model parameters)
  - Which features should you incorporate?
  - Support design decisions with real examples
  - State which extension 

Report
  - Problem Setting
  - Sequence Tagging Model
  - Extensions
  - Individual Member Contributions

Grading
  Proposal - 10
  Implementation - 50
  Report - 40
  Required ext (1) - 10
  Option - 10

1 list of sentences (dic reviews)
2 plot features (but we don't know our features yet)


List of experimetns to run
1. Test with implemented models and our model using same feature set
2. Test with different packages (nltk and mallet)
3. Use different classifiers and report accuracies, f-measure 
4. Use different models - HMM, MEMMS and CRFS
5. Use different feature sets - think about more features - can we use sentiWord?