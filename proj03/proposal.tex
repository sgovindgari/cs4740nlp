%!TEX TS-program = pdflatex or xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass{article}
\usepackage[letterpaper, top=1in, left=0.75in, right=0.75in, bottom=0.75in] {geometry}
\usepackage{microtype} % micro appearance
\usepackage[all]{nowidow} % no lone lines
\usepackage{changepage} % changes layout mid-page
\usepackage{enumitem} % enum (itemize, enumerate, description)
\usepackage{ifthen}
\usepackage{xfrac}
\usepackage{fancyhdr} % headers
\usepackage{amsmath} % matrices
\usepackage{amssymb} % symbols
\usepackage{relsize}
\usepackage{gensymb} % symbols
\usepackage{tikz} % graphics?
\usepackage{bm} % bold math
\usepackage{multicol} % multiple columns
\usepackage{lipsum} % lipsum

\pagestyle{fancy}

\newcommand {\classname} {CS 4740/5740 -- Intro to Natural Language Processing}
\newcommand {\duedate} {Tue, 2014--04--15}
\newcommand {\hwtype} {Project}
\newcommand {\hwnum} {3 (proposal)}
\newcommand {\innermargin} {0.15in} % indentation amt of nested lst

\let\tss\textsuperscript
\let\oldunderline\underline
\renewcommand\underline[1]{\oldunderline{\smash{#1}}}

\fancyhead[L] {\ifthenelse {\thepage=1}
  {bgs53 (Ben Shulman), sg754 (Spandana Govindgari),\\
   ms969 (MJ Sun), amw275 (Andy Wang)}
  {\hwtype\ \hwnum\ (bgs53, sg754, ms969, amw275)}}
\fancyhead[R] {\ifthenelse {\thepage=1}
  {\classname\\\hwtype\ \hwnum\ (due \duedate)}
  {Page \thepage}}

\begin{document}
\begin{center}\textbf{Project 3: Sequence tagging; Sentiment Classification -- Proposal}\end{center}

Our code will be written in Python.

\subsection*{Our sequence-tagging system}

\textit{Describe your sequence-tagging system.}\par

\begin{itemize}
\item \textit{Which model are you planning to implement?}\par

We are not initially going to implement MEMMs as our sequence tagging system; we will implement HMMs. We do not plan on using HMM libraries, but instead will write our own.

For our baseline we will use the following algorithm. From the training data we will record how many times each word occurs in a positive, neutral or negative sentence. Using that we can estimate the probability of a sentence's sentiment. For each word in the sentence we pull its probability for each sentiment and multiply it into the probability for each sentiment. Thus we perform the following equation to classify the sentence, where $S$ is the set of sentiments and the sentence has $n$ tokens: $argmax_{s \in S}\mathlarger{\prod_{i=1}^n} P(s|w_i)$.

\item \textit{Explain the algorithmic key points of your model. (Hidden variables \& observed variables for our setting, model parameters)}\par

Our hidden variables are the sentiments (-1, 0, 1). Our observed variables are the feature vectors of sentences. Our HMM Model will take in a set of tags (sentiments), a transition probability matrix, an emission matrix, the start state and what n-gram model to use. By n-gram model we mean, is the transmission matrix bi-gram, tri-gram, etc.

\item \textit{Brainstorm which features you would incorporate to learn emission prob-
abilities. Support your design decisions based on the real examples given
in the dataset.}\par

\end{itemize}

\subsection*{Schedule}

\begin{itemize}[noitemsep,nolistsep]
\item date: event
\end{itemize}

Rough division of labor:
\begin{itemize}[noitemsep,nolistsep]
\item name: task
\end{itemize}

\end{document}
